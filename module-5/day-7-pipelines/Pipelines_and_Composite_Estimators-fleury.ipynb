{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src= \"img/pipelines.png\" style=\"height:450px\">\n",
    "\n",
    "\n",
    "[Image Source](https://towardsdatascience.com/using-functiontransformer-and-pipeline-in-sklearn-to-predict-chardonnay-ratings-9b13fdd6c6fd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Agenda__\n",
    "\n",
    "- Pipelines and Composite estimators\n",
    "\n",
    "- Why do we need them?\n",
    "\n",
    "- How to use them in sklearn: accessing a particular object in pipe and changing parameters\n",
    "\n",
    "- Combining pipelines with gridsearch\n",
    "\n",
    "- Summary and further reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Objective defined\n",
    "- Data Cleaning\n",
    "- Preprocessing before modeling data:\n",
    "    - Scale\n",
    "    - One hot encode\n",
    "    - Imputing data for missing values/ '0' or '99'\n",
    "    - PCA\n",
    "    - Feature engineering\n",
    "        - Polynomial, log (either to X or Y), square, interaction terms\n",
    "    - Feature selection\n",
    "    \n",
    "- **Can do the preprocessing in a pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, Normalizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines\n",
    "\n",
    "__What is a Pipeline?__\n",
    "\n",
    "_Transformers:_ Any object with .transform method. Ex: PCA, OneHotEncoder.\n",
    "\n",
    "_Estimators:_ Any object with predict method. Ex: RandomForestClassifier, LinearRegression etc.\n",
    "\n",
    "_Pipelines:_ A tool for combining transformers with estimators. \n",
    "\n",
    "\n",
    "__Other relevant tools are:__\n",
    "\n",
    "- FutureUnion\n",
    "\n",
    "- TransformedTargetRegressor\n",
    "\n",
    "__Why do we need pipelines?__\n",
    "\n",
    "- Convenience and encapsulation\n",
    "\n",
    "Even though we train 10 transformer and 5 estimator we will call fit and predict once.\n",
    "\n",
    "- Joint parameter selection - here emphasize preprocessing part\n",
    "- Not only for the model, but also for the preprocessing transformers (very helpful!)\n",
    "\n",
    "We can put pipelines into gridsearch and find best parameters for all the estimators at once.\n",
    "\n",
    "- Safety\n",
    "\n",
    "Pipelines help avoid leaking statistics from your test data into trained model.\n",
    "\n",
    "[Pipelines and Composite Estimators](https://scikit-learn.org/stable/modules/compose.html#combining-estimators)\n",
    "\n",
    "\n",
    "\n",
    "## Usage of Pipelines\n",
    "\n",
    "The Pipeline is built using a list of (key, value) pairs, where the key is a string containing the name you want to give this step and value is an estimator object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('imputer',\n",
       "                 SimpleImputer(add_indicator=False, copy=True, fill_value=None,\n",
       "                               missing_values=nan, strategy='mean',\n",
       "                               verbose=0)),\n",
       "                ('reduce_dim',\n",
       "                 PCA(copy=True, iterated_power='auto', n_components=None,\n",
       "                     random_state=None, svd_solver='auto', tol=0.0,\n",
       "                     whiten=False)),\n",
       "                ('clf',\n",
       "                 SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None,\n",
       "                     coef0=0.0, decision_function_shape='ovr', degree=3,\n",
       "                     gamma='scale', kernel='rbf', max_iter=-1,\n",
       "                     probability=False, random_state=None, shrinking=True,\n",
       "                     tol=0.001, verbose=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "estimators = [('imputer', SimpleImputer()),('reduce_dim', PCA()), ('clf', SVC())]\n",
    "\n",
    "pipe = Pipeline(estimators)\n",
    "\n",
    "pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your Turn__\n",
    "\n",
    "- Create your own pipeline. You can use the same transformers and estimators with different parameters and 'name'.\n",
    "\n",
    "- You can create a new pipe with a scaler also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 1-5 supplement.py\n",
    "pipe = Pipeline([('imputer', SimpleImputer()),\n",
    "                 ('scaler', StandardScaler()),\n",
    "                 ('clf', LogisticRegression(C = 1000,\n",
    "                                                max_iter = 1000,\n",
    "                                                solver = 'saga'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe2 = make_pipeline(SimpleImputer(), StandardScaler(), LogisticRegression(C = 1000,\n",
    "                                                max_iter = 1000,\n",
    "                                                solver = 'saga'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sklearn also gives us \"make_pipeline\" which is almost the same thing but with make_pipeline you don't have to give names.\n",
    "\n",
    "__Your Turn__\n",
    "\n",
    "-  [Check documentation: 6.1.1.1.1. Construction](https://scikit-learn.org/stable/modules/compose.html) and use make_pipeline to construct an pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('simpleimputer',\n",
       "                 SimpleImputer(add_indicator=False, copy=True, fill_value=None,\n",
       "                               missing_values=nan, strategy='mean',\n",
       "                               verbose=0)),\n",
       "                ('standardscaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('logisticregression',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='auto', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %load -r 26-28 supplement.py\n",
    "pipe2 = make_pipeline(SimpleImputer(), StandardScaler(), LogisticRegression())\n",
    "\n",
    "pipe2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing steps\n",
    "\n",
    "We have multiple ways to access and object in the pipeline\n",
    "\n",
    "- steps attribute\n",
    "\n",
    "- [idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleImputer(add_indicator=False, copy=True, fill_value=None,\n",
       "              missing_values=nan, strategy='mean', verbose=0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## note that these will all give the simple imputer object\n",
    "pipe.steps[0]\n",
    "\n",
    "pipe['simpleimputer']\n",
    "\n",
    "pipe[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleImputer(add_indicator=False, copy=True, fill_value=None,\n",
       "              missing_values=nan, strategy='mean', verbose=0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## We can also access a particular object by named_steps\n",
    "## sklearn claims that tab completion should work here but \n",
    "## in my notebook it didn't\n",
    "\n",
    "pipe.named_steps.simpleimputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('standardscaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('logisticregression',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='auto', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## We can 'slice' pipelines to create sub-pipes\n",
    "\n",
    "pipe[1:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access to the parameters\n",
    "\n",
    "Parameters of the estimators in the pipeline can be accessed using the \n",
    "\"estimator__parameter\" syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('imputer',\n",
       "                 SimpleImputer(add_indicator=False, copy=True, fill_value=None,\n",
       "                               missing_values=nan, strategy='mean',\n",
       "                               verbose=0)),\n",
       "                ('reduce_dim',\n",
       "                 PCA(copy=True, iterated_power='auto', n_components=None,\n",
       "                     random_state=None, svd_solver='auto', tol=0.0,\n",
       "                     whiten=False)),\n",
       "                ('clf',\n",
       "                 SVC(C=10, break_ties=False, cache_size=200, class_weight=None,\n",
       "                     coef0=0.0, decision_function_shape='ovr', degree=3,\n",
       "                     gamma='scale', kernel='rbf', max_iter=-1,\n",
       "                     probability=False, random_state=None, shrinking=True,\n",
       "                     tol=0.001, verbose=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.set_params(clf__C = 10)\n",
    "\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('imputer',\n",
       "                 SimpleImputer(add_indicator=False, copy=True, fill_value=None,\n",
       "                               missing_values=nan, strategy='median',\n",
       "                               verbose=0)),\n",
       "                ('reduce_dim',\n",
       "                 PCA(copy=True, iterated_power='auto', n_components=None,\n",
       "                     random_state=None, svd_solver='auto', tol=0.0,\n",
       "                     whiten=False)),\n",
       "                ('clf',\n",
       "                 SVC(C=10, break_ties=False, cache_size=100, class_weight=None,\n",
       "                     coef0=0.0, decision_function_shape='ovr', degree=3,\n",
       "                     gamma='scale', kernel='rbf', max_iter=-1,\n",
       "                     probability=False, random_state=None, shrinking=True,\n",
       "                     tol=0.001, verbose=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change the parameters of the pipeline after defining them\n",
    "pipe.set_params(clf__cache_size=100)\n",
    "pipe.set_params(imputer__strategy = 'median')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching Transformers\n",
    "\n",
    "[(6.1.1.3. Caching transformers: avoid repeated computation)](https://scikit-learn.org/stable/modules/compose.html#pipeline-chaining-estimators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Storing the trained transformers. Useful when doing GridSearch so it won't have to do it again for every iteration.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways of caching transformers:\n",
    "\n",
    "1. Use mkdtemp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "\n",
    "X, y = load_boston(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory='/var/folders/j4/x_vwn42s7nn6tdn2j0r9ljb40000gn/T/tmpc6m28hh9',\n",
       "         steps=[('reduce_dim',\n",
       "                 PCA(copy=True, iterated_power='auto', n_components=None,\n",
       "                     random_state=None, svd_solver='auto', tol=0.0,\n",
       "                     whiten=False)),\n",
       "                ('clf',\n",
       "                 LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "                                  normalize=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tempfile import mkdtemp\n",
    "from shutil import rmtree\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "estimators = [('reduce_dim', PCA()), ('clf', LinearRegression())]\n",
    "\n",
    "# create temp folder that will store: PCA, LinearRegression\n",
    "cachedir = mkdtemp()\n",
    "pipe = Pipeline(estimators, memory=cachedir) \n",
    "pipe.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the cache directory when you don't need it anymore\n",
    "rmtree(cachedir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or by giving a string as the directory names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory='cached_transformers',\n",
       "         steps=[('reduce_dim',\n",
       "                 PCA(copy=True, iterated_power='auto', n_components=None,\n",
       "                     random_state=None, svd_solver='auto', tol=0.0,\n",
       "                     whiten=False)),\n",
       "                ('clf',\n",
       "                 LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "                                  normalize=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = Pipeline(estimators, memory='cached_transformers')\n",
    "pipe.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## again remove it when you are done with them\n",
    "rmtree('cached_transformers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**However, won't be able to access coefficients, params if cached. Will need to use another method.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming target in regression: when you want to make a trans to the target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipelines cannot transform on Y variables. Will need to import TransformedTargetRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score: 0.67\n",
      "R2 score: 0.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/flatironschooldc2/opt/anaconda3/envs/learn-env/lib/python3.6/site-packages/sklearn/preprocessing/_data.py:2357: UserWarning: n_quantiles (1000) is greater than the total number of samples (379). n_quantiles is set to n_samples.\n",
      "  % (self.n_quantiles, n_samples))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = load_boston(return_X_y=True)\n",
    "\n",
    "transformer = QuantileTransformer(output_distribution='normal')\n",
    "regressor = LinearRegression()\n",
    "regr = TransformedTargetRegressor(regressor=regressor,\n",
    "                                  transformer=transformer)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# applies transformation to y\n",
    "regr.fit(X_train, y_train)\n",
    "\n",
    "# has inverse transform already included\n",
    "print('R2 score: {0:.2f}'.format(regr.score(X_test, y_test)))\n",
    "\n",
    "raw_target_regr = LinearRegression().fit(X_train, y_train)\n",
    "print('R2 score: {0:.2f}'.format(raw_target_regr.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([23.27152702, 23.4       , 33.2       , 10.9       , 21.4       ,\n",
       "       19.62021138, 21.2       , 21.1       , 19.2       , 20.1       ,\n",
       "        5.        , 14.9       , 17.4       ,  6.65004785, 50.        ,\n",
       "       34.61034559, 22.7       , 44.34453699, 32.9867652 , 22.5       ,\n",
       "       23.9       , 24.20172669, 20.05133096, 31.70093439, 22.        ,\n",
       "       12.7       , 17.8       , 18.58001694, 43.42653849, 20.04348536,\n",
       "       18.2       , 18.4       , 19.9404876 , 22.57570934, 29.6       ,\n",
       "       20.6       ,  8.5       , 25.        , 17.86265554, 14.5       ,\n",
       "       25.28479391, 20.1       , 21.41578664, 16.08486244, 21.7       ,\n",
       "       23.97642227, 20.1       , 22.8       ,  9.36997255, 23.2       ,\n",
       "       21.27308207, 16.60345899, 23.2       , 30.79044804, 14.31310147,\n",
       "       20.42443604, 19.86302251, 15.17376169, 11.20175978, 22.13741921,\n",
       "       17.60180754, 20.9       , 36.16350157, 34.83675554, 17.8       ,\n",
       "       35.4       , 18.87210869, 18.5       , 17.8       , 22.3       ,\n",
       "       22.61377789, 23.7       , 32.        , 28.46615071, 26.48289248,\n",
       "        5.95043823, 48.59842066, 22.9       , 25.        , 19.3       ,\n",
       "       28.67081497, 19.8301377 , 19.5       , 50.        , 50.        ,\n",
       "       23.1       , 23.1       , 16.0386183 , 25.16448571, 16.70405514,\n",
       "       14.3       , 12.6291146 , 23.1       , 32.22935921, 22.3       ,\n",
       "       20.1       ,  5.        , 23.21389832, 15.6       , 18.18417542,\n",
       "       24.1       , 21.7       , 36.69760757, 22.        , 25.        ,\n",
       "       22.44671024,  7.        , 16.7       , 22.2       , 29.054841  ,\n",
       "       43.38599721, 13.08346635, 19.21951554, 20.1       , 11.10381715,\n",
       "       23.1       ,  6.30269709, 20.8       ,  8.81013033, 50.        ,\n",
       "       32.19614454, 12.66487849, 17.72869903, 20.77204188, 23.1       ,\n",
       "       21.2       , 39.6970657 ])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# When you want to see your predictions, you want to see Y on its original scale\n",
    "# does reverse transformation so that you get Y values in orginal scale (i.e. not log or squared output)\n",
    "regr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FeatureUnion: Parallelized version of pipeline, instead of step by step process. All steps in a union are applied to the same dataset, then bundles the result.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pipelines with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('simpleimputer',\n",
       "                 SimpleImputer(add_indicator=False, copy=True, fill_value=None,\n",
       "                               missing_values=nan, strategy='mean',\n",
       "                               verbose=0)),\n",
       "                ('standardscaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('logisticregression',\n",
       "                 LogisticRegression(C=1000, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=1000,\n",
       "                                    multi_class='auto', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='saga', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('simpleimputer',\n",
       "                                        SimpleImputer(add_indicator=False,\n",
       "                                                      copy=True,\n",
       "                                                      fill_value=None,\n",
       "                                                      missing_values=nan,\n",
       "                                                      strategy='mean',\n",
       "                                                      verbose=0)),\n",
       "                                       ('standardscaler',\n",
       "                                        StandardScaler(copy=True,\n",
       "                                                       with_mean=True,\n",
       "                                                       with_std=True)),\n",
       "                                       ('logisticregression',\n",
       "                                        LogisticRegression(C=1000,\n",
       "                                                           class_weight=None,\n",
       "                                                           dual=False,\n",
       "                                                           fit...\n",
       "                                                           max_iter=1000,\n",
       "                                                           multi_class='auto',\n",
       "                                                           n_jobs=None,\n",
       "                                                           penalty='l2',\n",
       "                                                           random_state=None,\n",
       "                                                           solver='saga',\n",
       "                                                           tol=0.0001,\n",
       "                                                           verbose=0,\n",
       "                                                           warm_start=False))],\n",
       "                                verbose=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'LogisticRegression__C': [0.1, 10, 100],\n",
       "                         'simpleimputer__strategy': ['mean', 'median']},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## we can also access the parameters in the gridsearch\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = dict(simpleimputer__strategy=['mean', 'median'],\n",
    "                  LogisticRegression__C=[0.1, 10, 100]) # or c = np.logspace   (better)\n",
    "grid_search = GridSearchCV(pipe2, param_grid=param_grid, cv=5)\n",
    "# since CV (cross-validation) =5 -> fits a total of 30 models\n",
    "\n",
    "grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## note also that we can choose to skip some steps in the pipeline \n",
    "\n",
    "param_grid = dict(standardscaler=['passthrough', PCA(5), PCA(6)], # 'passthrough' will use the default or pre-defined value as well\n",
    "                  clf=[SVC(gamma = 'auto'), LogisticRegression(solver = 'lbfgs',max_iter =1000)], # can define 2 models: SVC, LR\n",
    "                  svc__C=[0.1, 10, 100])\n",
    "\n",
    "grid_search = GridSearchCV(pipe2, param_grid=param_grid)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(768, 9)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv('data/diabetes.csv')\n",
    "display(df.head(),df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']\n"
     ]
    }
   ],
   "source": [
    "target = df.Outcome\n",
    "column_list = df.columns.tolist()\n",
    "\n",
    "column_list.remove('Outcome')\n",
    "print(column_list)\n",
    "\n",
    "data = df[column_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[On Scaling data](https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                target,\n",
    "                test_size=0.20,\n",
    "                stratify= target,\n",
    "                random_state = 120919)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your Turn__\n",
    "\n",
    "- Create a pipeline and use this pipeline for fitting and predicting diabetes results for the above data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory='cached_transformers',\n",
       "         steps=[('reduce_dim',\n",
       "                 PCA(copy=True, iterated_power='auto', n_components=None,\n",
       "                     random_state=None, svd_solver='auto', tol=0.0,\n",
       "                     whiten=False)),\n",
       "                ('clf',\n",
       "                 LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "                                  normalize=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimators = [('reduce_dim', PCA(n_components=5)), ('clf', SVC())]\n",
    "my_pipe = Pipeline(estimators)\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pipe = [('ss', StandardScaler()), ('PCA', PCA()), clf = [('clf', SVC()), ('log_reg', LogisticRegression())]]\n",
    "grid_search = GridSearchCV(my_pipe, param_grid=param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 1-10 supplement.py\n",
    "pipe = Pipeline([('imputer', SimpleImputer()),\n",
    "                 ('scaler', StandardScaler()),\n",
    "                 ('clf', LogisticRegression(C = 1000,\n",
    "                                                max_iter = 1000,\n",
    "                                                solver = 'saga'))])\n",
    "\n",
    "## we can access to a particular step in the pipeline\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your Turn__\n",
    "\n",
    "- Now use gridsearch with pipelines and return the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -r 14-23 supplement.py\n",
    "param_grid = dict(scaler=['passthrough',MinMaxScaler() , Normalizer(), StandardScaler()],\n",
    "                  clf = [SVC(gamma = 'auto'),\n",
    "                         LogisticRegression(solver = 'lbfgs',max_iter =1000)],\n",
    "                  clf__C = [0.1, 10, 100])\n",
    "\n",
    "gs = GridSearchCV(pipe, param_grid=param_grid, cv = 3, verbose = 1)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Remark__\n",
    "\n",
    "\n",
    "Note that even if gridsearch and pipes are getting along very well. The options are not limitless. Try to add Randomforests as classifier in the gridsearch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further research and miscellaneous\n",
    "\n",
    "- [FeatureUnion](https://scikit-learn.org/stable/modules/compose.html#featureunion-composite-feature-spaces)\n",
    "\n",
    "- [ColumnTransformer](https://scikit-learn.org/stable/modules/compose.html#columntransformer-for-heterogeneous-data)\n",
    "\n",
    "- [sklearn, dictionary of terms](https://scikit-learn.org/stable/glossary.html#term-transformer)\n",
    "\n",
    "- [Pydata meeting on pipelines](https://www.youtube.com/watch?v=BFaadIqWlAg)\n",
    "\n",
    "- [Another pydata talk on pipelines with FeatureUnion](https://www.youtube.com/watch?v=URdnFlZnlaE)\n",
    "\n",
    "- [On scalers](https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-scaler)\n",
    "\n",
    "- [A nice notebook on pipelines](https://github.com/amueller/introduction_to_ml_with_python/blob/master/06-algorithm-chains-and-pipelines.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

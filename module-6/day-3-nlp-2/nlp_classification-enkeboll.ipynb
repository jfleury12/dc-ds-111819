{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting progressbar2\n",
      "  Using cached https://files.pythonhosted.org/packages/16/68/adc395e0a3c86571081c8a2e2daaa5b58270f6854276a089a0e9b5fa2c33/progressbar2-3.47.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.7/site-packages (from progressbar2) (1.12.0)\n",
      "Collecting python-utils>=2.3.0 (from progressbar2)\n",
      "  Using cached https://files.pythonhosted.org/packages/eb/a0/19119d8b7c05be49baf6c593f11c432d571b70d805f2fe94c0585e55e4c8/python_utils-2.3.0-py2.py3-none-any.whl\n",
      "Installing collected packages: python-utils, progressbar2\n",
      "Successfully installed progressbar2-3.47.0 python-utils-2.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install progressbar2\n",
    "from progressbar import progressbar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to scrape reddit page (takes a reddit .json url)\n",
    "# returns posts \n",
    "\n",
    "def scraper_bike(url):\n",
    "    headers = {'User-Agent' : 'override this bad boy!'}\n",
    "    posts = []\n",
    "    after = {}\n",
    "\n",
    "    for page in progressbar(range(40)):\n",
    "        params = {'after': after}\n",
    "        pagepull = requests.get(url=url, params=params, headers=headers)\n",
    "        page_dict = pagepull.json()\n",
    "        posts.extend(page_dict['data']['children'])\n",
    "        after = page_dict['data']['after']\n",
    "        time.sleep(.2)\n",
    "        \n",
    "    return posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert posts to DataFrame - won't allow duplicate posts since unique id 'name' is set as index\n",
    "# Extract: name (as index) and subreddit, selftext, title (as columns)\n",
    "\n",
    "def posts_to_df(post_list):\n",
    "    i = 0\n",
    "    post_dict = {}\n",
    "    \n",
    "    for post in post_list:\n",
    "        ind = post_list[i]['data']\n",
    "        post_dict[ind['name']] = [ind['subreddit'], ind['title'], ind['selftext']]\n",
    "        i += 1\n",
    "\n",
    "    df_name = pd.DataFrame(post_dict)\n",
    "    df_name = df_name.T\n",
    "    df_name.columns = ['subreddit', 'title', 'selftext']\n",
    "    \n",
    "    return df_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes scraper function and url - outputs dataframe\n",
    "\n",
    "def scrape_to_df(scrape_func, url):\n",
    "    \n",
    "    return posts_to_df(scrape_func(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### If you want to scrape repeatedly over time and add to a csv\n",
    "# scrape, import csv, concat, drop duplicate, and output to csv\n",
    "# takes in scraper function, url, csv filename to import, csv filename to output\n",
    "# Outputs - Concatenated DataFrame as csv\n",
    "\n",
    "def scrape_add(scrape_func, url, import_file, export_file):\n",
    "    scrape_df = posts_to_df(scrape_func(url))\n",
    "    imported_df = pd.read_csv(import_file, index_col = 'Unnamed: 0')\n",
    "    concat_df = pd.concat([imported_df, scrape_df])\n",
    "    concat_df = concat_df[~concat_df.index.duplicated(keep='first')]\n",
    "    concat_df.to_csv(export_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (40 of 40) |########################| Elapsed Time: 0:00:20 Time:  0:00:20\n",
      "100% (40 of 40) |########################| Elapsed Time: 0:00:22 Time:  0:00:22\n"
     ]
    }
   ],
   "source": [
    "# You can also put in any 2 subreddits in as the URL and get results for those\n",
    "\n",
    "nfltest = scrape_to_df(scraper_bike, 'https://www.reddit.com/r/nfl.json')\n",
    "nbatest = scrape_to_df(scraper_bike, 'https://www.reddit.com/r/nba.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (40 of 40) |########################| Elapsed Time: 0:00:25 Time:  0:00:25\n",
      "100% (40 of 40) |########################| Elapsed Time: 0:00:22 Time:  0:00:22\n"
     ]
    }
   ],
   "source": [
    "politics_test = scrape_to_df(scraper_bike, 'https://www.reddit.com/r/politics.json')\n",
    "conservative_test = scrape_to_df(scraper_bike, 'https://www.reddit.com/r/conservative.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(785, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbatest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(913, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nfltest.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### These scrape_add functions add to already built csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape_add(scraper_bike, 'https://www.reddit.com/r/CollegeBasketball/new.json', 'NCAA_Posts_Update2.csv', 'NCAA_Posts_Update3.csv')\n",
    "# scrape_add(scraper_bike, 'https://www.reddit.com/r/AskScience/new.json', 'AskSci_Posts_Update2.csv', 'AskSci_Posts_Update3.csv')\n",
    "# scrape_add(scraper_bike, 'https://www.reddit.com/r/nba/new.json', 'NBA_Posts_Update2.csv', 'NBA_Posts_Update3.csv')\n",
    "# scrape_add(scraper_bike, 'https://www.reddit.com/r/nfl/new.json', 'NFL_Posts_Update2.csv', 'NFL_Posts_Update3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning / Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "pd.set_option('max_colwidth', 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>t3_f2b7iv</td>\n",
       "      <td>nfl</td>\n",
       "      <td>Talko Tuesday</td>\n",
       "      <td>Welcome to today's open thread, where /r/nfl users can discuss anything they wish not related directly to the NFL.\\n\\nWant to talk about personal life? Cool things about your fandom? Whatever happens to be dominating today's news cycle? Do you have something to talk about that didn't warrant its...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t3_f1ucwm</td>\n",
       "      <td>nfl</td>\n",
       "      <td>r/NFL Top 100 of the 2019 Season - CALL FOR RANKERS</td>\n",
       "      <td>Greetings /r/NFL! \\n\\nThe preeminent Top 100 is starting again this year and I am returning as host for the second consecutive year!   \\n  \\nIn order to continue with this now **5 week** list (**two** weekly reveal posts) and with the things we will do with it, we have to start ***NOW***. So wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t3_f2sslg</td>\n",
       "      <td>nfl</td>\n",
       "      <td>[Withers] AP source says Browns star DE Myles Garrett to be reinstated by NFL from suspension today.</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t3_f2i0bo</td>\n",
       "      <td>nfl</td>\n",
       "      <td>[Emmanuel Sanders] ðŸ˜‚ Its crazy how people keep talking bout my age but Iâ€™m still flying past 23 year olds and only had 2 drops the entire year. 1 with the broncos and one with the niners. Keep bringing up my age to make yourself feel good but go look at the film.</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t3_f2r86r</td>\n",
       "      <td>nfl</td>\n",
       "      <td>Vikings OC Gary Kubiak excited to be back calling plays: â€œI never lost the joy. I just said, 'Coach, do you mind if I go back home and mow some grass for a couple days or something to think about it?â€™ I think I called him in about a day and a half. I was just really excited to do it.\"</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t3_ey0hik</td>\n",
       "      <td>nfl</td>\n",
       "      <td>[Fortuna] The last three teams to enter the fourth quarter of the Super Bowl with a double-digit deficit have all won.</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t3_eyel2c</td>\n",
       "      <td>nfl</td>\n",
       "      <td>What, in your opinion, is the biggest snub for Superbowl MVP?</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t3_ey1nof</td>\n",
       "      <td>nfl</td>\n",
       "      <td>Mahomes becomes the first QB in NFL history to win the Super Bowl, SB MVP, NFL MVP, and FIRST TEAM ALL PRO in his first two seasons as a starter</td>\n",
       "      <td>And no other QB since Marino has even come close to this</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t3_exy5di</td>\n",
       "      <td>nfl</td>\n",
       "      <td>[Highlight] Jimmy G throws up a duck, Breeland picks it off</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t3_eym84e</td>\n",
       "      <td>nfl</td>\n",
       "      <td>If the NFL goes to 18 games, where do the extra 2 come from?</td>\n",
       "      <td>Just curious. Would they be conference games? Cross conference games? Divisional games?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>913 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          subreddit  \\\n",
       "t3_f2b7iv       nfl   \n",
       "t3_f1ucwm       nfl   \n",
       "t3_f2sslg       nfl   \n",
       "t3_f2i0bo       nfl   \n",
       "t3_f2r86r       nfl   \n",
       "...             ...   \n",
       "t3_ey0hik       nfl   \n",
       "t3_eyel2c       nfl   \n",
       "t3_ey1nof       nfl   \n",
       "t3_exy5di       nfl   \n",
       "t3_eym84e       nfl   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                   title  \\\n",
       "t3_f2b7iv                                                                                                                                                                                                                                                                                  Talko Tuesday   \n",
       "t3_f1ucwm                                                                                                                                                                                                                                            r/NFL Top 100 of the 2019 Season - CALL FOR RANKERS   \n",
       "t3_f2sslg                                                                                                                                                                                           [Withers] AP source says Browns star DE Myles Garrett to be reinstated by NFL from suspension today.   \n",
       "t3_f2i0bo                        [Emmanuel Sanders] ðŸ˜‚ Its crazy how people keep talking bout my age but Iâ€™m still flying past 23 year olds and only had 2 drops the entire year. 1 with the broncos and one with the niners. Keep bringing up my age to make yourself feel good but go look at the film.   \n",
       "t3_f2r86r  Vikings OC Gary Kubiak excited to be back calling plays: â€œI never lost the joy. I just said, 'Coach, do you mind if I go back home and mow some grass for a couple days or something to think about it?â€™ I think I called him in about a day and a half. I was just really excited to do it.\"   \n",
       "...                                                                                                                                                                                                                                                                                                  ...   \n",
       "t3_ey0hik                                                                                                                                                                         [Fortuna] The last three teams to enter the fourth quarter of the Super Bowl with a double-digit deficit have all won.   \n",
       "t3_eyel2c                                                                                                                                                                                                                                  What, in your opinion, is the biggest snub for Superbowl MVP?   \n",
       "t3_ey1nof                                                                                                                                               Mahomes becomes the first QB in NFL history to win the Super Bowl, SB MVP, NFL MVP, and FIRST TEAM ALL PRO in his first two seasons as a starter   \n",
       "t3_exy5di                                                                                                                                                                                                                                    [Highlight] Jimmy G throws up a duck, Breeland picks it off   \n",
       "t3_eym84e                                                                                                                                                                                                                                   If the NFL goes to 18 games, where do the extra 2 come from?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                              selftext  \n",
       "t3_f2b7iv  Welcome to today's open thread, where /r/nfl users can discuss anything they wish not related directly to the NFL.\\n\\nWant to talk about personal life? Cool things about your fandom? Whatever happens to be dominating today's news cycle? Do you have something to talk about that didn't warrant its...  \n",
       "t3_f1ucwm  Greetings /r/NFL! \\n\\nThe preeminent Top 100 is starting again this year and I am returning as host for the second consecutive year!   \\n  \\nIn order to continue with this now **5 week** list (**two** weekly reveal posts) and with the things we will do with it, we have to start ***NOW***. So wit...  \n",
       "t3_f2sslg                                                                                                                                                                                                                                                                                                               \n",
       "t3_f2i0bo                                                                                                                                                                                                                                                                                                               \n",
       "t3_f2r86r                                                                                                                                                                                                                                                                                                               \n",
       "...                                                                                                                                                                                                                                                                                                                ...  \n",
       "t3_ey0hik                                                                                                                                                                                                                                                                                                               \n",
       "t3_eyel2c                                                                                                                                                                                                                                                                                                               \n",
       "t3_ey1nof                                                                                                                                                                                                                                                     And no other QB since Marino has even come close to this  \n",
       "t3_exy5di                                                                                                                                                                                                                                                                                                               \n",
       "t3_eym84e                                                                                                                                                                                                                      Just curious. Would they be conference games? Cross conference games? Divisional games?  \n",
       "\n",
       "[913 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nfltest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop column\n",
    "\n",
    "nfltest = nfltest.drop(columns = 'selftext')\n",
    "nbatest = nbatest.drop(columns = 'selftext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge subreddit data\n",
    "\n",
    "train = pd.concat([nfltest, nbatest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>t3_f1bkps</td>\n",
       "      <td>nba</td>\n",
       "      <td>[Highlight] 17 years ago today, Jordan and Kobe battled and trash-talked during Mikeâ€™s final All-Star Game with no fear from either side</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t3_f20izj</td>\n",
       "      <td>nba</td>\n",
       "      <td>Whatâ€™s the best duo from this century where both players wonâ€™t make the Hall of Fame?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t3_f21sb2</td>\n",
       "      <td>nba</td>\n",
       "      <td>Nick young or J.R. ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t3_f23bsh</td>\n",
       "      <td>nba</td>\n",
       "      <td>How do technical foul free throws factor into TS%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t3_f2gcrr</td>\n",
       "      <td>nba</td>\n",
       "      <td>Embiid Discussion - Can someone knowledgable please help me out with some clarification around the possibility of a trade post deadline?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t3_f1s91b</td>\n",
       "      <td>nba</td>\n",
       "      <td>What seed/how far would this team go in the West if they were coached by Brad Stevens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t3_f1m0cb</td>\n",
       "      <td>nba</td>\n",
       "      <td>People need to understand the NBA and team sports in general are about matchups.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t3_f1nv9u</td>\n",
       "      <td>nba</td>\n",
       "      <td>Do you think the Raptors have a shot of taking out the Bucks in the playoffs?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t3_f20oyn</td>\n",
       "      <td>nba</td>\n",
       "      <td>Other than Lakers, Clippers, and Bucks, who else has a solid chance at the finals?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t3_f190nf</td>\n",
       "      <td>nba</td>\n",
       "      <td>Request: Iâ€™m trying to find that video clip of the Timberwolves players, one at a time, listening to music with headphones on. Thereâ€™s some hip hop, then I think country, then something a bit...different. Wiggins reaction is priceless and I canâ€™t seem to find it!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          subreddit  \\\n",
       "t3_f1bkps       nba   \n",
       "t3_f20izj       nba   \n",
       "t3_f21sb2       nba   \n",
       "t3_f23bsh       nba   \n",
       "t3_f2gcrr       nba   \n",
       "...             ...   \n",
       "t3_f1s91b       nba   \n",
       "t3_f1m0cb       nba   \n",
       "t3_f1nv9u       nba   \n",
       "t3_f20oyn       nba   \n",
       "t3_f190nf       nba   \n",
       "\n",
       "                                                                                                                                                                                                                                                                             title  \n",
       "t3_f1bkps                                                                                                                                 [Highlight] 17 years ago today, Jordan and Kobe battled and trash-talked during Mikeâ€™s final All-Star Game with no fear from either side  \n",
       "t3_f20izj                                                                                                                                                                                    Whatâ€™s the best duo from this century where both players wonâ€™t make the Hall of Fame?  \n",
       "t3_f21sb2                                                                                                                                                                                                                                                     Nick young or J.R. ?  \n",
       "t3_f23bsh                                                                                                                                                                                                                        How do technical foul free throws factor into TS%  \n",
       "t3_f2gcrr                                                                                                                                 Embiid Discussion - Can someone knowledgable please help me out with some clarification around the possibility of a trade post deadline?  \n",
       "...                                                                                                                                                                                                                                                                            ...  \n",
       "t3_f1s91b                                                                                                                                                                                    What seed/how far would this team go in the West if they were coached by Brad Stevens  \n",
       "t3_f1m0cb                                                                                                                                                                                         People need to understand the NBA and team sports in general are about matchups.  \n",
       "t3_f1nv9u                                                                                                                                                                                            Do you think the Raptors have a shot of taking out the Bucks in the playoffs?  \n",
       "t3_f20oyn                                                                                                                                                                                       Other than Lakers, Clippers, and Bucks, who else has a solid chance at the finals?  \n",
       "t3_f190nf  Request: Iâ€™m trying to find that video clip of the Timberwolves players, one at a time, listening to music with headphones on. Thereâ€™s some hip hop, then I think country, then something a bit...different. Wiggins reaction is priceless and I canâ€™t seem to find it!  \n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[-100:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenize (grab only word characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['title'] = train['title'].map(lambda x: word_tokenizer.tokenize(x.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rejoin list of tokenized words into single string for each row\n",
    "\n",
    "train['title'] = train['title'].map(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>t3_f2b7iv</td>\n",
       "      <td>nfl</td>\n",
       "      <td>talko tuesday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t3_f1ucwm</td>\n",
       "      <td>nfl</td>\n",
       "      <td>r nfl top 100 of the 2019 season call for rankers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t3_f2sslg</td>\n",
       "      <td>nfl</td>\n",
       "      <td>withers ap source says browns star de myles garrett to be reinstated by nfl from suspension today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t3_f2i0bo</td>\n",
       "      <td>nfl</td>\n",
       "      <td>emmanuel sanders its crazy how people keep talking bout my age but i m still flying past 23 year olds and only had 2 drops the entire year 1 with the broncos and one with the niners keep bringing up my age to make yourself feel good but go look at the film</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t3_f2r86r</td>\n",
       "      <td>nfl</td>\n",
       "      <td>vikings oc gary kubiak excited to be back calling plays i never lost the joy i just said coach do you mind if i go back home and mow some grass for a couple days or something to think about it i think i called him in about a day and a half i was just really excited to do it</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          subreddit  \\\n",
       "t3_f2b7iv       nfl   \n",
       "t3_f1ucwm       nfl   \n",
       "t3_f2sslg       nfl   \n",
       "t3_f2i0bo       nfl   \n",
       "t3_f2r86r       nfl   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                        title  \n",
       "t3_f2b7iv                                                                                                                                                                                                                                                                       talko tuesday  \n",
       "t3_f1ucwm                                                                                                                                                                                                                                   r nfl top 100 of the 2019 season call for rankers  \n",
       "t3_f2sslg                                                                                                                                                                                   withers ap source says browns star de myles garrett to be reinstated by nfl from suspension today  \n",
       "t3_f2i0bo                    emmanuel sanders its crazy how people keep talking bout my age but i m still flying past 23 year olds and only had 2 drops the entire year 1 with the broncos and one with the niners keep bringing up my age to make yourself feel good but go look at the film  \n",
       "t3_f2r86r  vikings oc gary kubiak excited to be back calling plays i never lost the joy i just said coach do you mind if i go back home and mow some grass for a couple days or something to think about it i think i called him in about a day and a half i was just really excited to do it  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['title'][0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split and converting series to list of strings then to array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train[['title']]\n",
    "y = train['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=.25,\n",
    "                                                    random_state=42,\n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nfl    0.537691\n",
       "nba    0.462309\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# baseline is\n",
    "\n",
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our training data list - this is a list of strings, with each string being a post title\n",
    "\n",
    "clean_train_data = []\n",
    "\n",
    "for traindata in X_train['title']:\n",
    "    clean_train_data.append(traindata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1273"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xfl ticket prices in washington are higher than comparable redskins tickets',\n",
       " 'zach lavine 41 points 8 threes full highlights bulls vs wizards february 11 2020',\n",
       " 'despite having been selected to first team all pro once 2018 and second team all pro three times 2019 2017 2016 chiefs rt mitchell schwartz has never gone to a pro bowl',\n",
       " 'if the nfl goes to 18 games where do the extra 2 come from',\n",
       " 'kansas city celebration detected by national weather service radar']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_train_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test data list\n",
    "\n",
    "clean_test_data = []\n",
    "\n",
    "for testdata in X_test['title']:\n",
    "    clean_test_data.append(testdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clean_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate our CountVectorizer. This counts the number of appearances of all the words in our training data and\n",
    "# eliminates common english stop words. 5000 max features works well for our purposes (tested various numbers). Our\n",
    "# data is already preprocessed and tokenized manually earlier. ngram_range is 1,3, although all or nearly all our\n",
    "# features are single words\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer=\"word\",\n",
    "                             tokenizer=None,\n",
    "                             preprocessor=None,\n",
    "                             stop_words='english',\n",
    "                             max_features=5000,\n",
    "                             ngram_range=(1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit our training data and test data lists to our count_vectorizer\n",
    "\n",
    "train_data_features = vectorizer.fit_transform(clean_train_data)\n",
    "\n",
    "test_data_features = vectorizer.transform(clean_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to array\n",
    "\n",
    "train_data_features = train_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1273, 5000), (425, 5000))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check shapes\n",
    "\n",
    "train_data_features.shape, test_data_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# I wanted check that the features corpus was as expected - removed print statement for readability\n",
    "\n",
    "vocab = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['000',\n",
       " '02',\n",
       " '02 06',\n",
       " '06',\n",
       " '07',\n",
       " '10',\n",
       " '10 2020',\n",
       " '10 fgm',\n",
       " '10 game',\n",
       " '10 game span',\n",
       " '10 games',\n",
       " '10 nfl',\n",
       " '10 points',\n",
       " '10 seasons',\n",
       " '100',\n",
       " '100 nfl',\n",
       " '100 pass',\n",
       " '100 pass blocking',\n",
       " '101',\n",
       " '101 players',\n",
       " '101 players 2019',\n",
       " '103',\n",
       " '105',\n",
       " '106',\n",
       " '10th',\n",
       " '10th straight',\n",
       " '11',\n",
       " '11 2020',\n",
       " '11 points',\n",
       " '111',\n",
       " '113',\n",
       " '114',\n",
       " '116',\n",
       " '116 105',\n",
       " '11th',\n",
       " '12',\n",
       " '120',\n",
       " '123',\n",
       " '126',\n",
       " '13',\n",
       " '13 kansas',\n",
       " '13 kansas city',\n",
       " '133',\n",
       " '133 92',\n",
       " '135',\n",
       " '14',\n",
       " '140',\n",
       " '142',\n",
       " '15',\n",
       " '15 39',\n",
       " '15 game',\n",
       " '15 game winning',\n",
       " '15 games',\n",
       " '150',\n",
       " '150 teams',\n",
       " '150 teams 2020',\n",
       " '15th',\n",
       " '16',\n",
       " '17',\n",
       " '17 18',\n",
       " '17th',\n",
       " '18',\n",
       " '18 33',\n",
       " '18 million',\n",
       " '18 points',\n",
       " '19',\n",
       " '19 35',\n",
       " '1981',\n",
       " '1986',\n",
       " '1996',\n",
       " '19th',\n",
       " '1st',\n",
       " '1st round',\n",
       " '1st round pick',\n",
       " '1st team',\n",
       " '20',\n",
       " '20 point',\n",
       " '20 points',\n",
       " '20 years',\n",
       " '200',\n",
       " '2000',\n",
       " '2004',\n",
       " '2006',\n",
       " '2009',\n",
       " '2010',\n",
       " '2010s',\n",
       " '2011',\n",
       " '2012',\n",
       " '2013',\n",
       " '2015',\n",
       " '2016',\n",
       " '2017',\n",
       " '2018',\n",
       " '2019',\n",
       " '2019 nfl',\n",
       " '2019 nfl season',\n",
       " '2019 season',\n",
       " '2020',\n",
       " '2020 21',\n",
       " '2020 black',\n",
       " '2020 black majority',\n",
       " '2020 draft',\n",
       " '2020 dunk',\n",
       " '2020 dunk contest',\n",
       " '2020 freedawkins',\n",
       " '2020 nfl',\n",
       " '2020 nfl draft',\n",
       " '2020 olympics',\n",
       " '2020 season',\n",
       " '2020 star',\n",
       " '2021',\n",
       " '2022',\n",
       " '207',\n",
       " '21',\n",
       " '21 32',\n",
       " '21 defeat',\n",
       " '21 february',\n",
       " '21 february 11',\n",
       " '21 points',\n",
       " '22',\n",
       " '22 31',\n",
       " '22 31 february',\n",
       " '22 points',\n",
       " '23',\n",
       " '23 31',\n",
       " '24',\n",
       " '25',\n",
       " '26',\n",
       " '26 6m',\n",
       " '27',\n",
       " '28',\n",
       " '28 points',\n",
       " '29',\n",
       " '29th',\n",
       " '2nd',\n",
       " '2nd chance',\n",
       " '30',\n",
       " '30 feet',\n",
       " '300m',\n",
       " '30th',\n",
       " '31',\n",
       " '31 17',\n",
       " '31 february',\n",
       " '31 points',\n",
       " '31 pts',\n",
       " '31m',\n",
       " '32',\n",
       " '32 16',\n",
       " '32 21',\n",
       " '33',\n",
       " '33 21',\n",
       " '34',\n",
       " '35',\n",
       " '35 18',\n",
       " '36',\n",
       " '36 points',\n",
       " '37',\n",
       " '37 16',\n",
       " '38',\n",
       " '39',\n",
       " '39 points',\n",
       " '3m',\n",
       " '3p',\n",
       " '3pt',\n",
       " '3pt assists',\n",
       " '3pt reb',\n",
       " '3pt reb ast',\n",
       " '3rd',\n",
       " '3rd 15',\n",
       " '40',\n",
       " '40 wins',\n",
       " '400',\n",
       " '41',\n",
       " '41 points',\n",
       " '42',\n",
       " '43',\n",
       " '44',\n",
       " '45',\n",
       " '46',\n",
       " '48',\n",
       " '48 13',\n",
       " '49',\n",
       " '49ers',\n",
       " '49ers 13',\n",
       " '49ers 13 kansas',\n",
       " '49ers george',\n",
       " '49ers george kittle',\n",
       " '49ers jimmy',\n",
       " '49ers loss',\n",
       " '4q',\n",
       " '4q defensive',\n",
       " '4q defensive epa',\n",
       " '4th',\n",
       " '4th quarter',\n",
       " '50',\n",
       " '500',\n",
       " '500 yards',\n",
       " '51',\n",
       " '52',\n",
       " '53']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[0:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODELING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit logistic regression model\n",
    "\n",
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1273, 5000), (1273,))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape check\n",
    "\n",
    "train_data_features.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(train_data_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.997643362136685"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(train_data_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9152941176470588"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(test_data_features, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates a dataframe that matches features to coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_list = lr.coef_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_list = coef_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df = pd.DataFrame({'features': vectorizer.get_feature_names(),\n",
    "                        'coefs': coef_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>coefs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1942</td>\n",
       "      <td>nba</td>\n",
       "      <td>-2.231317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>911</td>\n",
       "      <td>embiid</td>\n",
       "      <td>-1.289872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1471</td>\n",
       "      <td>knicks</td>\n",
       "      <td>-1.216775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1222</td>\n",
       "      <td>highlight</td>\n",
       "      <td>-1.213947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4911</td>\n",
       "      <td>westbrook</td>\n",
       "      <td>-1.105111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1017</td>\n",
       "      <td>football</td>\n",
       "      <td>1.216912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>super</td>\n",
       "      <td>1.242598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>611</td>\n",
       "      <td>chiefs</td>\n",
       "      <td>1.283339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3218</td>\n",
       "      <td>qb</td>\n",
       "      <td>1.539649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1975</td>\n",
       "      <td>nfl</td>\n",
       "      <td>2.904114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       features     coefs\n",
       "1942        nba -2.231317\n",
       "911      embiid -1.289872\n",
       "1471     knicks -1.216775\n",
       "1222  highlight -1.213947\n",
       "4911  westbrook -1.105111\n",
       "...         ...       ...\n",
       "1017   football  1.216912\n",
       "4600      super  1.242598\n",
       "611      chiefs  1.283339\n",
       "3218         qb  1.539649\n",
       "1975        nfl  2.904114\n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_df.sort_values(by = ['coefs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's throw out these unfair words and rerun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "extra_stopwords = ['nba', 'basketball', 'football', 'nfl']\n",
    "\n",
    "stopwords.update(extra_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1273, 5000), (425, 5000))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(analyzer = \"word\",\n",
    "                             tokenizer = None,\n",
    "                             preprocessor = None,\n",
    "                             stop_words = stopwords,\n",
    "                             max_features = 5000,\n",
    "                             ngram_range = (1, 3))\n",
    "\n",
    "train_data_features = vectorizer.fit_transform(clean_train_data)\n",
    "\n",
    "test_data_features = vectorizer.transform(clean_test_data)\n",
    "\n",
    "train_data_features = train_data_features.toarray()\n",
    "\n",
    "train_data_features.shape, test_data_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(train_data_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9913589945011784"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(train_data_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8941176470588236"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(test_data_features, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>coefs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>4216</td>\n",
       "      <td>td</td>\n",
       "      <td>0.235329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     features     coefs\n",
       "4216       td  0.235329"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_list = lr.coef_.tolist()\n",
    "coef_list = coef_list[0]\n",
    "\n",
    "coef_df = pd.DataFrame({'features' : vectorizer.get_feature_names(),\n",
    "                       'coefs' : coef_list})\n",
    "\n",
    "coef_df.sort_values(by = ['coefs'])\n",
    "coef_df.loc[coef_df.features == 'td']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(max_features=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                       max_features=100, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort=False,\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.fit(train_data_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.997643362136685"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.score(train_data_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7223529411764706"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.score(test_data_features, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest.fit(train_data_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.997643362136685"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest.score(train_data_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8870588235294118"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest.score(test_data_features, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Matrix on Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr.predict(test_data_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_df = pd.DataFrame(cm,\n",
    "                    columns=['predict_neg', 'predict_pos'],\n",
    "                    index = ['actual_neg', 'actual_pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predict_neg</th>\n",
       "      <th>predict_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>actual_neg</td>\n",
       "      <td>176</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>actual_pos</td>\n",
       "      <td>25</td>\n",
       "      <td>204</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            predict_neg  predict_pos\n",
       "actual_neg          176           20\n",
       "actual_pos           25          204"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking where our model failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df = pd.DataFrame({'y_actual' : y_test,\n",
    "             'y_predicted' : y_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "mismatch_df = comparison_df[comparison_df['y_actual'] != comparison_df['y_predicted']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "mismatch2_df = pd.concat([mismatch_df, X_test], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All incorrect predictions with titles\n",
    "\n",
    "mismatches = mismatch2_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_actual</th>\n",
       "      <th>y_predicted</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>t3_ey09cu</td>\n",
       "      <td>nfl</td>\n",
       "      <td>nba</td>\n",
       "      <td>highlight williams runs it in for the td</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t3_ey0p03</td>\n",
       "      <td>nfl</td>\n",
       "      <td>nba</td>\n",
       "      <td>highlight how bout those chieeeeeeeeeeefs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t3_eyazqy</td>\n",
       "      <td>nfl</td>\n",
       "      <td>nba</td>\n",
       "      <td>kosko mitchell schwartz had a postseason grade of 92 8 the best grade of any player at any position he allowed just one pressure across three games</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t3_eyh22q</td>\n",
       "      <td>nfl</td>\n",
       "      <td>nba</td>\n",
       "      <td>highlight the 49er faithful cheer on the team as they arrive in the bay area</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t3_eyt34u</td>\n",
       "      <td>nfl</td>\n",
       "      <td>nba</td>\n",
       "      <td>talko tuesday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t3_ez0d39</td>\n",
       "      <td>nfl</td>\n",
       "      <td>nba</td>\n",
       "      <td>what if the nfl hockey fied penalties</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t3_ez1ba5</td>\n",
       "      <td>nfl</td>\n",
       "      <td>nba</td>\n",
       "      <td>weirdest nfl rivalries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t3_ezem3q</td>\n",
       "      <td>nfl</td>\n",
       "      <td>nba</td>\n",
       "      <td>duval coalition formed opposition to removing jacksonville home games</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t3_ezgbbt</td>\n",
       "      <td>nfl</td>\n",
       "      <td>nba</td>\n",
       "      <td>what is sutton trade value</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t3_ezhbqh</td>\n",
       "      <td>nfl</td>\n",
       "      <td>nba</td>\n",
       "      <td>players tribune what the hell happened to vince young by vince young</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t3_ezjy31</td>\n",
       "      <td>nfl</td>\n",
       "      <td>nba</td>\n",
       "      <td>raheem mostert contemplated putting this on ebay but this belongs to you and yours you did your thing hand this down for generations proud of you bro much love undraftedrbs tooloose26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t3_ezm8iv</td>\n",
       "      <td>nfl</td>\n",
       "      <td>nba</td>\n",
       "      <td>who is the most underrated player in your team s history</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t3_f03dlj</td>\n",
       "      <td>nfl</td>\n",
       "      <td>nba</td>\n",
       "      <td>michael vick 30 for 30 part 2 thread anyone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t3_f0ileh</td>\n",
       "      <td>nfl</td>\n",
       "      <td>nba</td>\n",
       "      <td>who would you put on your favorite team s all 2010 s starting lineup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t3_f0mncu</td>\n",
       "      <td>nfl</td>\n",
       "      <td>nba</td>\n",
       "      <td>what s the most significant injury to a player that was caused by somebody on his team either in a game or practice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t3_f0rsro</td>\n",
       "      <td>nfl</td>\n",
       "      <td>nba</td>\n",
       "      <td>constructing a 53 man roster from all 50 states dc and two territories</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t3_f0vz52</td>\n",
       "      <td>nfl</td>\n",
       "      <td>nba</td>\n",
       "      <td>top 10 nfl players of all time by points scored nfl rankings by career points scored 1921 2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t3_f0xudl</td>\n",
       "      <td>nfl</td>\n",
       "      <td>nba</td>\n",
       "      <td>who was more impressive 1999 2003 greatest show on turf or 2012 2016 legion of boom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t3_f12hoe</td>\n",
       "      <td>nfl</td>\n",
       "      <td>nba</td>\n",
       "      <td>what game would look like a blowout one way if you looked at total yards but was in fact a blowout the other way</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t3_f1aiqz</td>\n",
       "      <td>nfl</td>\n",
       "      <td>nba</td>\n",
       "      <td>michael irvin i never said jerry or anyone in the organization said this to me it was not anyone with the dallascowboys</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t3_f1imej</td>\n",
       "      <td>nba</td>\n",
       "      <td>nfl</td>\n",
       "      <td>furkan korkmaz has become the first 76er in the 2019 20 season to score 30 points in consecutive games</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t3_f1n0ij</td>\n",
       "      <td>nfl</td>\n",
       "      <td>nba</td>\n",
       "      <td>would you rather have your team go 8 8 or 0 0 16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t3_f1ouj3</td>\n",
       "      <td>nba</td>\n",
       "      <td>nfl</td>\n",
       "      <td>and the shaqademy award goes to shaqtin a fool episode 12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t3_f1wzt1</td>\n",
       "      <td>nba</td>\n",
       "      <td>nfl</td>\n",
       "      <td>d lo and kat are giving away 1k tickets each</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t3_f20xy7</td>\n",
       "      <td>nba</td>\n",
       "      <td>nfl</td>\n",
       "      <td>of the 10 youngest teams in the nba 7 are in the bottom 10 for net rating and two more have negative net ratings the one remaining team the boston celtics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t3_f21ffl</td>\n",
       "      <td>nfl</td>\n",
       "      <td>nba</td>\n",
       "      <td>la canfora hearing it is far from a foregone conclusion that terrell suggs retires weighing his options still loves the game could pursue a one year deal in the right spot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t3_f221ic</td>\n",
       "      <td>nba</td>\n",
       "      <td>nfl</td>\n",
       "      <td>when players are wearing protective facemasks is there a rule that says they have to be clear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t3_f259pt</td>\n",
       "      <td>nba</td>\n",
       "      <td>nfl</td>\n",
       "      <td>top 5 offensive and top 5 defensive teams before asb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t3_f27zae</td>\n",
       "      <td>nba</td>\n",
       "      <td>nfl</td>\n",
       "      <td>trade talk tuesday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t3_f29fsk</td>\n",
       "      <td>nba</td>\n",
       "      <td>nfl</td>\n",
       "      <td>vanvleet on current win streak the regular season that s a no brainer for us we know that we can do that we do that in our sleep i don t want us to say it s easy because i don t want to be disrespectful but we can do that for us it s about being healthy and continuing to get better</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t3_f2a3cp</td>\n",
       "      <td>nba</td>\n",
       "      <td>nfl</td>\n",
       "      <td>whitaker dwane casey calls out sekou doumbouya i m very concerned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t3_f2ac7a</td>\n",
       "      <td>nba</td>\n",
       "      <td>nfl</td>\n",
       "      <td>what moves would you make this offseason if you were the 76ers gm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t3_f2b2bn</td>\n",
       "      <td>nba</td>\n",
       "      <td>nfl</td>\n",
       "      <td>is nurkic a top 5 center when healthy do you think he ll return to that level this season</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t3_f2b7iv</td>\n",
       "      <td>nfl</td>\n",
       "      <td>nba</td>\n",
       "      <td>talko tuesday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t3_f2bf2l</td>\n",
       "      <td>nba</td>\n",
       "      <td>nfl</td>\n",
       "      <td>is there a team right now that is just stuck meaning no overly negative contracts neither an shortage or abundance of assets yet still not much cap space and not much current or projected future success</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t3_f2bohu</td>\n",
       "      <td>nba</td>\n",
       "      <td>nfl</td>\n",
       "      <td>how are everyone s rookies doing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t3_f2c9mv</td>\n",
       "      <td>nba</td>\n",
       "      <td>nfl</td>\n",
       "      <td>the nba should reward teams with cash and other incentives for how well they do in the regular season</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t3_f2dftk</td>\n",
       "      <td>nba</td>\n",
       "      <td>nfl</td>\n",
       "      <td>what s with this sub and its pretension in shitting on over the hill players</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t3_f2eivw</td>\n",
       "      <td>nba</td>\n",
       "      <td>nfl</td>\n",
       "      <td>quick question</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t3_f2f6z3</td>\n",
       "      <td>nba</td>\n",
       "      <td>nfl</td>\n",
       "      <td>which team would you take if everyone is in his prime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t3_f2jazk</td>\n",
       "      <td>nba</td>\n",
       "      <td>nfl</td>\n",
       "      <td>is thj being snubbed from the 3pt co test because the nba had to include lavine since it s in chicago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t3_f2lhbf</td>\n",
       "      <td>nfl</td>\n",
       "      <td>nba</td>\n",
       "      <td>new jeremy hill video is raheem mostert an elite runningback</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t3_f2nn0t</td>\n",
       "      <td>nba</td>\n",
       "      <td>nfl</td>\n",
       "      <td>outsider perspective on the steph never gets calls take</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t3_f2o50r</td>\n",
       "      <td>nfl</td>\n",
       "      <td>nba</td>\n",
       "      <td>do you think some franchises just try to make money</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>t3_f2pg4y</td>\n",
       "      <td>nba</td>\n",
       "      <td>nfl</td>\n",
       "      <td>why is it that some teams who tank for several years for picks never end up being good</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          y_actual y_predicted  \\\n",
       "t3_ey09cu      nfl         nba   \n",
       "t3_ey0p03      nfl         nba   \n",
       "t3_eyazqy      nfl         nba   \n",
       "t3_eyh22q      nfl         nba   \n",
       "t3_eyt34u      nfl         nba   \n",
       "t3_ez0d39      nfl         nba   \n",
       "t3_ez1ba5      nfl         nba   \n",
       "t3_ezem3q      nfl         nba   \n",
       "t3_ezgbbt      nfl         nba   \n",
       "t3_ezhbqh      nfl         nba   \n",
       "t3_ezjy31      nfl         nba   \n",
       "t3_ezm8iv      nfl         nba   \n",
       "t3_f03dlj      nfl         nba   \n",
       "t3_f0ileh      nfl         nba   \n",
       "t3_f0mncu      nfl         nba   \n",
       "t3_f0rsro      nfl         nba   \n",
       "t3_f0vz52      nfl         nba   \n",
       "t3_f0xudl      nfl         nba   \n",
       "t3_f12hoe      nfl         nba   \n",
       "t3_f1aiqz      nfl         nba   \n",
       "t3_f1imej      nba         nfl   \n",
       "t3_f1n0ij      nfl         nba   \n",
       "t3_f1ouj3      nba         nfl   \n",
       "t3_f1wzt1      nba         nfl   \n",
       "t3_f20xy7      nba         nfl   \n",
       "t3_f21ffl      nfl         nba   \n",
       "t3_f221ic      nba         nfl   \n",
       "t3_f259pt      nba         nfl   \n",
       "t3_f27zae      nba         nfl   \n",
       "t3_f29fsk      nba         nfl   \n",
       "t3_f2a3cp      nba         nfl   \n",
       "t3_f2ac7a      nba         nfl   \n",
       "t3_f2b2bn      nba         nfl   \n",
       "t3_f2b7iv      nfl         nba   \n",
       "t3_f2bf2l      nba         nfl   \n",
       "t3_f2bohu      nba         nfl   \n",
       "t3_f2c9mv      nba         nfl   \n",
       "t3_f2dftk      nba         nfl   \n",
       "t3_f2eivw      nba         nfl   \n",
       "t3_f2f6z3      nba         nfl   \n",
       "t3_f2jazk      nba         nfl   \n",
       "t3_f2lhbf      nfl         nba   \n",
       "t3_f2nn0t      nba         nfl   \n",
       "t3_f2o50r      nfl         nba   \n",
       "t3_f2pg4y      nba         nfl   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                title  \n",
       "t3_ey09cu                                                                                                                                                                                                                                                    highlight williams runs it in for the td  \n",
       "t3_ey0p03                                                                                                                                                                                                                                                   highlight how bout those chieeeeeeeeeeefs  \n",
       "t3_eyazqy                                                                                                                                         kosko mitchell schwartz had a postseason grade of 92 8 the best grade of any player at any position he allowed just one pressure across three games  \n",
       "t3_eyh22q                                                                                                                                                                                                                highlight the 49er faithful cheer on the team as they arrive in the bay area  \n",
       "t3_eyt34u                                                                                                                                                                                                                                                                               talko tuesday  \n",
       "t3_ez0d39                                                                                                                                                                                                                                                       what if the nfl hockey fied penalties  \n",
       "t3_ez1ba5                                                                                                                                                                                                                                                                      weirdest nfl rivalries  \n",
       "t3_ezem3q                                                                                                                                                                                                                       duval coalition formed opposition to removing jacksonville home games  \n",
       "t3_ezgbbt                                                                                                                                                                                                                                                                  what is sutton trade value  \n",
       "t3_ezhbqh                                                                                                                                                                                                                        players tribune what the hell happened to vince young by vince young  \n",
       "t3_ezjy31                                                                                                     raheem mostert contemplated putting this on ebay but this belongs to you and yours you did your thing hand this down for generations proud of you bro much love undraftedrbs tooloose26  \n",
       "t3_ezm8iv                                                                                                                                                                                                                                    who is the most underrated player in your team s history  \n",
       "t3_f03dlj                                                                                                                                                                                                                                                 michael vick 30 for 30 part 2 thread anyone  \n",
       "t3_f0ileh                                                                                                                                                                                                                        who would you put on your favorite team s all 2010 s starting lineup  \n",
       "t3_f0mncu                                                                                                                                                                         what s the most significant injury to a player that was caused by somebody on his team either in a game or practice  \n",
       "t3_f0rsro                                                                                                                                                                                                                      constructing a 53 man roster from all 50 states dc and two territories  \n",
       "t3_f0vz52                                                                                                                                                                                              top 10 nfl players of all time by points scored nfl rankings by career points scored 1921 2019  \n",
       "t3_f0xudl                                                                                                                                                                                                         who was more impressive 1999 2003 greatest show on turf or 2012 2016 legion of boom  \n",
       "t3_f12hoe                                                                                                                                                                            what game would look like a blowout one way if you looked at total yards but was in fact a blowout the other way  \n",
       "t3_f1aiqz                                                                                                                                                                     michael irvin i never said jerry or anyone in the organization said this to me it was not anyone with the dallascowboys  \n",
       "t3_f1imej                                                                                                                                                                                      furkan korkmaz has become the first 76er in the 2019 20 season to score 30 points in consecutive games  \n",
       "t3_f1n0ij                                                                                                                                                                                                                                            would you rather have your team go 8 8 or 0 0 16  \n",
       "t3_f1ouj3                                                                                                                                                                                                                                   and the shaqademy award goes to shaqtin a fool episode 12  \n",
       "t3_f1wzt1                                                                                                                                                                                                                                                d lo and kat are giving away 1k tickets each  \n",
       "t3_f20xy7                                                                                                                                  of the 10 youngest teams in the nba 7 are in the bottom 10 for net rating and two more have negative net ratings the one remaining team the boston celtics  \n",
       "t3_f21ffl                                                                                                                 la canfora hearing it is far from a foregone conclusion that terrell suggs retires weighing his options still loves the game could pursue a one year deal in the right spot  \n",
       "t3_f221ic                                                                                                                                                                                               when players are wearing protective facemasks is there a rule that says they have to be clear  \n",
       "t3_f259pt                                                                                                                                                                                                                                        top 5 offensive and top 5 defensive teams before asb  \n",
       "t3_f27zae                                                                                                                                                                                                                                                                          trade talk tuesday  \n",
       "t3_f29fsk  vanvleet on current win streak the regular season that s a no brainer for us we know that we can do that we do that in our sleep i don t want us to say it s easy because i don t want to be disrespectful but we can do that for us it s about being healthy and continuing to get better  \n",
       "t3_f2a3cp                                                                                                                                                                                                                           whitaker dwane casey calls out sekou doumbouya i m very concerned  \n",
       "t3_f2ac7a                                                                                                                                                                                                                           what moves would you make this offseason if you were the 76ers gm  \n",
       "t3_f2b2bn                                                                                                                                                                                                   is nurkic a top 5 center when healthy do you think he ll return to that level this season  \n",
       "t3_f2b7iv                                                                                                                                                                                                                                                                               talko tuesday  \n",
       "t3_f2bf2l                                                                                  is there a team right now that is just stuck meaning no overly negative contracts neither an shortage or abundance of assets yet still not much cap space and not much current or projected future success  \n",
       "t3_f2bohu                                                                                                                                                                                                                                                            how are everyone s rookies doing  \n",
       "t3_f2c9mv                                                                                                                                                                                       the nba should reward teams with cash and other incentives for how well they do in the regular season  \n",
       "t3_f2dftk                                                                                                                                                                                                                what s with this sub and its pretension in shitting on over the hill players  \n",
       "t3_f2eivw                                                                                                                                                                                                                                                                              quick question  \n",
       "t3_f2f6z3                                                                                                                                                                                                                                       which team would you take if everyone is in his prime  \n",
       "t3_f2jazk                                                                                                                                                                                       is thj being snubbed from the 3pt co test because the nba had to include lavine since it s in chicago  \n",
       "t3_f2lhbf                                                                                                                                                                                                                                new jeremy hill video is raheem mostert an elite runningback  \n",
       "t3_f2nn0t                                                                                                                                                                                                                                     outsider perspective on the steph never gets calls take  \n",
       "t3_f2o50r                                                                                                                                                                                                                                         do you think some franchises just try to make money  \n",
       "t3_f2pg4y                                                                                                                                                                                                      why is it that some teams who tank for several years for picks never end up being good  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mismatches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Term Frequency / Inverse Document Frequency\n",
    "\n",
    "TF(w) = (Number of times term w appears in a document) / (Total number of terms in the document)\n",
    "\n",
    "IDF(w) = log_e(Total number of documents / Number of documents with term w in it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vec = TfidfVectorizer(analyzer=\"word\",\n",
    "                            tokenizer=None,\n",
    "                            preprocessor=None,\n",
    "                            stop_words=stopwords,\n",
    "                            max_features=5000,\n",
    "                            ngram_range=(1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1273, 5000), (425, 5000))"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_features = tfidf_vec.fit_transform(clean_train_data)\n",
    "\n",
    "test_data_features = tfidf_vec.transform(clean_test_data)\n",
    "\n",
    "train_data_features = train_data_features.toarray()\n",
    "\n",
    "train_data_features.shape, test_data_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(train_data_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9795758051846033"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(train_data_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9011764705882352"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(test_data_features, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try on some other subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([politics_test, conservative_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train[['title']]\n",
    "y = train['subreddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# politics_test = scrape_to_df(scraper_bike, 'https://www.reddit.com/r/politics.json')\n",
    "# conservative_test = scrape_to_df(scraper_bike, 'https://www.reddit.com/r/conservative.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "politics_test = politics_test.drop(columns='selftext')\n",
    "conservative_test = conservative_test.drop(columns='selftext')\n",
    "\n",
    "train = pd.concat([politics_test, conservative_test])\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "train['title'] = train['title'].map(lambda x: tokenizer.tokenize(x.lower()))\n",
    "train['title'] = train['title'].map(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our training data list - this is a list of strings, with each string being a post title\n",
    "\n",
    "clean_train_data = []\n",
    "\n",
    "for traindata in X_train['title']:\n",
    "    clean_train_data.append(traindata)\n",
    "    \n",
    "    \n",
    "# create test data list\n",
    "\n",
    "clean_test_data = []\n",
    "\n",
    "for testdata in X_test['title']:\n",
    "    clean_test_data.append(testdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer=\"word\",\n",
    "                             tokenizer=None,\n",
    "                             preprocessor=None,\n",
    "                             stop_words='english',\n",
    "                             max_features=5000,\n",
    "                             ngram_range=(1, 3))\n",
    "\n",
    "train_data_features = vectorizer.fit_transform(clean_train_data)\n",
    "\n",
    "test_data_features = vectorizer.transform(clean_test_data)\n",
    "\n",
    "train_data_features = train_data_features.toarray()\n",
    "\n",
    "train_data_features.shape, test_data_features.shape\n",
    "\n",
    "vocab = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(penalty = 'l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1297, 5000), (1297,))"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_features.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9830377794911334"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(train_data_features, y_train)\n",
    "\n",
    "lr.score(train_data_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7344110854503464"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(test_data_features, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_list = lr.coef_.tolist()\n",
    "\n",
    "coef_list = coef_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>coefs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>692</td>\n",
       "      <td>dems</td>\n",
       "      <td>-1.076319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2678</td>\n",
       "      <td>people</td>\n",
       "      <td>-1.053654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4956</td>\n",
       "      <td>won</td>\n",
       "      <td>-1.018671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4841</td>\n",
       "      <td>video</td>\n",
       "      <td>-1.007651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4462</td>\n",
       "      <td>socialism</td>\n",
       "      <td>-0.949057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1658</td>\n",
       "      <td>military</td>\n",
       "      <td>1.047845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>318</td>\n",
       "      <td>bloomberg</td>\n",
       "      <td>1.148311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4527</td>\n",
       "      <td>stone</td>\n",
       "      <td>1.214183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4698</td>\n",
       "      <td>trump</td>\n",
       "      <td>1.280603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>359</td>\n",
       "      <td>budget</td>\n",
       "      <td>1.501506</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       features     coefs\n",
       "692        dems -1.076319\n",
       "2678     people -1.053654\n",
       "4956        won -1.018671\n",
       "4841      video -1.007651\n",
       "4462  socialism -0.949057\n",
       "...         ...       ...\n",
       "1658   military  1.047845\n",
       "318   bloomberg  1.148311\n",
       "4527      stone  1.214183\n",
       "4698      trump  1.280603\n",
       "359      budget  1.501506\n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coef_df = pd.DataFrame({'features' : vectorizer.get_feature_names(),\n",
    "                       'coefs' : coef_list})\n",
    "\n",
    "coef_df.sort_values(by = ['coefs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... for, like, actual poets. By [Allison Parrish](http://www.decontextualize.com/)\n",
    "\n",
    "\n",
    "## Why word vectors?\n",
    "\n",
    "Poetry is, at its core, the art of identifying and manipulating linguistic similarity. I have discovered a truly marvelous proof of this, which this notebook is too narrow to contain. (By which I mean: I will elaborate on this some other time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Animal similarity and simple linear algebra\n",
    "\n",
    "We'll begin by considering a small subset of English: words for animals. Our task is to be able to write computer programs to find similarities among these words and the creatures they designate. To do this, we might start by making a spreadsheet of some animals and their characteristics. For example:\n",
    "\n",
    "![Animal spreadsheet](http://static.decontextualize.com/snaps/animal-spreadsheet.png)\n",
    "\n",
    "This spreadsheet associates a handful of animals with two numbers: their cuteness and their size, both in a range from zero to one hundred. (The values themselves are simply based on my own judgment. Your taste in cuteness and evaluation of size may differ significantly from mine. As with all data, these data are simply a mirror reflection of the person who collected them.)\n",
    "\n",
    "These values give us everything we need to make determinations about which animals are similar (at least, similar in the properties that we've included in the data). Try to answer the following question: Which animal is most similar to a capybara? You could go through the values one by one and do the math to make that evaluation, but visualizing the data as points in 2-dimensional space makes finding the answer very intuitive:\n",
    "\n",
    "![Animal space](http://static.decontextualize.com/snaps/animal-space.png)\n",
    "\n",
    "The plot shows us that the closest animal to the capybara is the panda bear (again, in terms of its subjective size and cuteness). One way of calculating how \"far apart\" two points are is to find their *Euclidean distance*. (This is simply the length of the line that connects the two points.) For points in two dimensions, Euclidean distance can be calculated with the following Python function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def distance2d(x1, y1, x2, y2):\n",
    "    return np.linalg.norm(np.array([x1, y1])-np.array([x2, y2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the distance between \"capybara\" (70, 30) and \"panda\" (74, 40):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.180339887498949"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance2d(70, 30, 75, 40) # panda and capybara"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... is less than the distance between \"tarantula\" and \"elephant\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104.0096149401583"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance2d(8, 3, 65, 90) # tarantula and elephant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modeling animals in this way has a few other interesting properties. For example, you can pick an arbitrary point in \"animal space\" and then find the animal closest to that point. If you imagine an animal of size 25 and cuteness 30, you can easily look at the space to find the animal that most closely fits that description: the chicken.\n",
    "\n",
    "Reasoning visually, you can also answer questions like: what's halfway between a chicken and an elephant? Simply draw a line from \"elephant\" to \"chicken,\" mark off the midpoint and find the closest animal. (According to our chart, halfway between an elephant and a chicken is a horse.)\n",
    "\n",
    "You can also ask: what's the *difference* between a hamster and a tarantula? According to our plot, it's about seventy five units of cute (and a few units of size).\n",
    "\n",
    "The relationship of \"difference\" is an interesting one, because it allows us to reason about *analogous* relationships. In the chart below, I've drawn an arrow from \"tarantula\" to \"hamster\" (in blue):\n",
    "\n",
    "![Animal analogy](http://static.decontextualize.com/snaps/animal-space-analogy.png)\n",
    "\n",
    "You can understand this arrow as being the *relationship* between a tarantula and a hamster, in terms of their size and cuteness (i.e., hamsters and tarantulas are about the same size, but hamsters are much cuter). In the same diagram, I've also transposed this same arrow (this time in red) so that its origin point is \"chicken.\" The arrow ends closest to \"kitten.\" What we've discovered is that the animal that is about the same size as a chicken but much cuter is... a kitten. To put it in terms of an analogy:\n",
    "\n",
    "    Tarantulas are to hamsters as chickens are to kittens.\n",
    "    \n",
    "A sequence of numbers used to identify a point is called a *vector*, and the kind of math we've been doing so far is called *linear algebra.* (Linear algebra is surprisingly useful across many domains: It's the same kind of math you might do to, e.g., simulate the velocity and acceleration of a sprite in a video game.)\n",
    "\n",
    "A set of vectors that are all part of the same data set is often called a *vector space*. The vector space of animals in this section has two *dimensions*, by which I mean that each vector in the space has two numbers associated with it (i.e., two columns in the spreadsheet). The fact that this space has two dimensions just happens to make it easy to *visualize* the space by drawing a 2D plot. But most vector spaces you'll work with will have more than two dimensionsâ€”sometimes many hundreds. In those cases, it's more difficult to visualize the \"space,\" but the math works pretty much the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language with vectors: colors\n",
    "\n",
    "So far, so good. We have a system in placeâ€”albeit highly subjectiveâ€”for talking about animals and the words used to name them. I want to talk about another vector space that has to do with language: the vector space of colors.\n",
    "\n",
    "Colors are often represented in computers as vectors with three dimensions: red, green, and blue. Just as with the animals in the previous section, we can use these vectors to answer questions like: which colors are similar? What's the most likely color name for an arbitrarily chosen set of values for red, green and blue? Given the names of two colors, what's the name of those colors' \"average\"?\n",
    "\n",
    "We'll be working with this [color data](https://github.com/dariusk/corpora/blob/master/data/colors/xkcd.json) from the [xkcd color survey](https://blog.xkcd.com/2010/05/03/color-survey-results/). The data relates a color name to the RGB value associated with that color. [Here's a page that shows what the colors look like](https://xkcd.com/color/rgb/). Download the color data and put it in the same directory as this notebook.\n",
    "\n",
    "A few notes before we proceed:\n",
    "\n",
    "* The linear algebra functions implemented below (`addv`, `meanv`, etc.) are slow, potentially inaccurate, and shouldn't be used for \"real\" codeâ€”I wrote them so beginner programmers can understand how these kinds of functions work behind the scenes. Use [numpy](http://www.numpy.org/) for fast and accurate math in Python.\n",
    "* If you're interested in perceptually accurate color math in Python, consider using the [colormath library](http://python-colormath.readthedocs.io/en/latest/).\n",
    "\n",
    "Now, import the `json` library and load the color data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = requests.get('https://raw.githubusercontent.com/dariusk/corpora/master/data/colors/xkcd.json')\n",
    "color_data = resp.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function converts colors from hex format (`#1a2b3c`) to a tuple of integers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hex_to_int(s):\n",
    "    return int(s[1:3], 16), int(s[3:5], 16), int(s[5:7], 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the following cell creates a dictionary and populates it with mappings from color names to RGB vectors for each color in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = dict()\n",
    "for item in color_data['colors']:\n",
    "    colors[item[\"color\"]] = hex_to_int(item[\"hex\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(110, 117, 14)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colors['olive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(229, 0, 0)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colors['red']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0, 0)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colors['black']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector math\n",
    "\n",
    "Before we keep going, we'll need some functions for performing basic vector arithmetic. These functions will work with vectors in spaces of any number of dimensions.\n",
    "\n",
    "The first function returns the Euclidean distance between two points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0990195135927845"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "def distance(coord1, coord2):\n",
    "    return np.linalg.norm(np.array(coord1)-np.array(coord2))\n",
    "distance([10, 1], [5, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `subtractv` function subtracts one vector from another:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, -1]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def subtractv(coord1, coord2):\n",
    "    return list(np.array(coord1) - np.array(coord2))\n",
    "subtractv([10, 1], [5, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `addv` vector adds two vectors together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15, 3]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def addv(coord1, coord2):\n",
    "    return list(np.array(coord1) + np.array(coord2))\n",
    "addv([10, 1], [5, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the `meanv` function takes a list of vectors and finds their mean or average:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.0, 2.0]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def meanv(coords):\n",
    "    return list(np.mean(coords, axis=0))\n",
    "meanv([[0, 1], [2, 2], [4, 3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as a test, the following cell shows that the distance from \"red\" to \"green\" is greater than the distance from \"red\" to \"pink\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance(colors['red'], colors['green']) > distance(colors['red'], colors['pink'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the closest item\n",
    "\n",
    "Just as we wanted to find the animal that most closely matched an arbitrary point in cuteness/size space, we'll want to find the closest color name to an arbitrary point in RGB space. The easiest way to find the closest item to an arbitrary vector is simply to find the distance between the target vector and each item in the space, in turn, then sort the list from closest to farthest. The `closest()` function below does just that. By default, it returns a list of the ten closest items to the given vector.\n",
    "\n",
    "> Note: Calculating \"closest neighbors\" like this is fine for the examples in this notebook, but unmanageably slow for vector spaces of any appreciable size. As your vector space grows, you'll want to move to a faster solution, like SciPy's [kdtree](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.spatial.KDTree.html) or [Annoy](https://pypi.python.org/pypi/annoy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest(space, coord, n=10):\n",
    "    return sorted(space.keys(), key=lambda x: distance(coord, space[x]))[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing it out, we can find the ten colors closest to \"red\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['white',\n",
       " 'pale grey',\n",
       " 'very light pink',\n",
       " 'off white',\n",
       " 'ice blue',\n",
       " 'very pale blue',\n",
       " 'ice',\n",
       " 'very light blue',\n",
       " 'really light blue',\n",
       " 'eggshell']"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closest(colors, colors['white'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... or the ten colors closest to (150, 60, 150):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bubble gum pink',\n",
       " 'strong pink',\n",
       " 'electric pink',\n",
       " 'bubblegum',\n",
       " 'hot pink',\n",
       " 'coral pink',\n",
       " 'carnation pink',\n",
       " 'salmon',\n",
       " 'light red',\n",
       " 'pink']"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closest(colors, [5000, 60, 150])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Color magic\n",
    "\n",
    "The magical part of representing words as vectors is that the vector operations we defined earlier appear to operate on language the same way they operate on numbers. For example, if we find the word closest to the vector resulting from subtracting \"red\" from \"purple,\" we get a series of \"blue\" colors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dark red',\n",
       " 'blood',\n",
       " 'deep red',\n",
       " 'blood red',\n",
       " 'indian red',\n",
       " 'reddy brown',\n",
       " 'mahogany',\n",
       " 'dried blood',\n",
       " 'crimson',\n",
       " 'brick red']"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closest(colors, subtractv(colors['purple'], colors['blue']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matches our intuition about RGB colors, which is that purple is a combination of red and blue. Take away the red, and blue is all you have left.\n",
    "\n",
    "You can do something similar with addition. What's blue plus green?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bright turquoise',\n",
       " 'bright light blue',\n",
       " 'bright aqua',\n",
       " 'cyan',\n",
       " 'neon blue',\n",
       " 'aqua blue',\n",
       " 'bright cyan',\n",
       " 'bright sky blue',\n",
       " 'aqua',\n",
       " 'bright teal']"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closest(colors, addv(colors['blue'], colors['green']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's right, it's something like turquoise or cyan! What if we find the average of black and white? Predictably, we get gray:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['medium grey',\n",
       " 'purple grey',\n",
       " 'steel grey',\n",
       " 'battleship grey',\n",
       " 'grey purple',\n",
       " 'purplish grey',\n",
       " 'greyish purple',\n",
       " 'steel',\n",
       " 'warm grey',\n",
       " 'green grey']"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the average of black and white: medium grey\n",
    "closest(colors, meanv([colors['black'], colors['white']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as with the tarantula/hamster example from the previous section, we can use color vectors to reason about relationships between colors. In the cell below, finding the difference between \"pink\" and \"red\" then adding it to \"blue\" seems to give us a list of colors that are to blue what pink is to red (i.e., a slightly lighter, less saturated shade):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['forrest green',\n",
       " 'dark green',\n",
       " 'racing green',\n",
       " 'hunter green',\n",
       " 'very dark green',\n",
       " 'bottle green',\n",
       " 'darkgreen',\n",
       " 'dark forest green',\n",
       " 'dark olive',\n",
       " 'forest green']"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# an analogy: pink is to red as X is to blue\n",
    "pink_to_red = subtractv(colors['pink'], colors['red'])\n",
    "closest(colors, subtractv(pink_to_red, colors['blue']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example of color analogies: Navy is to blue as true green/dark grass green is to green:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['true green',\n",
       " 'dark grass green',\n",
       " 'grassy green',\n",
       " 'racing green',\n",
       " 'forest',\n",
       " 'bottle green',\n",
       " 'dark olive green',\n",
       " 'darkgreen',\n",
       " 'forrest green',\n",
       " 'grass green']"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# another example: \n",
    "navy_to_blue = subtractv(colors['navy'], colors['blue'])\n",
    "closest(colors, addv(navy_to_blue, colors['green']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The examples above are fairly simple from a mathematical perspective but nevertheless *feel* magical: they're demonstrating that it's possible to use math to reason about how people use language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interlude: A Love Poem That Loses Its Way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "red = colors['red']\n",
    "blue = colors['blue']\n",
    "for i in range(14):\n",
    "    rednames = closest(colors, red)\n",
    "    bluenames = closest(colors, blue)\n",
    "    print(f\"Roses are {rednames[0]}, violets are {bluenames[0]}\")\n",
    "    red = colors[random.choice(rednames[1:])]\n",
    "    blue = colors[random.choice(bluenames[1:])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doing bad digital humanities with color vectors\n",
    "\n",
    "With the tools above in hand, we can start using our vectorized knowledge of language toward academic ends. In the following example, I'm going to calculate the average color of Bram Stoker's *Dracula*.\n",
    "\n",
    "First, we'll load [spaCy](https://spacy.io/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b9/d8/26120ca4a5b1a6fe215ccb01185cee41098eb61386b3fd3a7b28c085146a/spacy-2.2.3-cp37-cp37m-macosx_10_6_intel.whl (14.2MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14.2MB 3.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Using cached https://files.pythonhosted.org/packages/b9/bd/faace403086ee922afc74e5615cb8c21020fcf5d5667314e943c08f71fde/murmurhash-1.0.2-cp37-cp37m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.7/site-packages (from spacy) (41.4.0)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Using cached https://files.pythonhosted.org/packages/c7/3e/9aaba1f8c0cb69e57ebeb411cc1b65b3f6bfc3572dd68969a6d3e59288f6/preshed-3.0.2-cp37-cp37m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl\n",
      "Collecting srsly<1.1.0,>=0.1.0 (from spacy)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/33/fa6fed9e0dbadc75f1f88780ae7a5182f3dd4019a007a7a787468965355e/srsly-1.0.1-cp37-cp37m-macosx_10_6_intel.whl (271kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 276kB 18.3MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.7/site-packages (from spacy) (2.22.0)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/34/a8b682ee9b57db35a5fe4e179b77c1bda0f6a09745669a99cfc27aa2bed7/cymem-2.0.3-cp37-cp37m-macosx_10_6_intel.whl (54kB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61kB 25.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting plac<1.2.0,>=0.9.6 (from spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/86/85/40b8f66c2dd8f4fd9f09d59b22720cffecf1331e788b8a0cab5bafb353d1/plac-1.1.3-py2.py3-none-any.whl\n",
      "Collecting blis<0.5.0,>=0.4.0 (from spacy)\n",
      "  Using cached https://files.pythonhosted.org/packages/85/d8/f0be9d8ebec9cbeea1427de6ac0ecc919c0bfe881eff2d2965dbc310ca8b/blis-0.4.1-cp37-cp37m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl\n",
      "Collecting wasabi<1.1.0,>=0.4.0 (from spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/21/e1/e4e7b754e6be3a79c400eb766fb34924a6d278c43bb828f94233e0124a21/wasabi-0.6.0-py3-none-any.whl\n",
      "Collecting thinc<7.4.0,>=7.3.0 (from spacy)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/87/c7da01c45bf3c138242f19f09b987d0ffa632ac7e8508527ee34d3ebbd81/thinc-7.3.1-cp37-cp37m-macosx_10_6_intel.whl (3.0MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.0MB 1.5MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.15.0 in /opt/anaconda3/lib/python3.7/site-packages (from spacy) (1.17.2)\n",
      "Collecting catalogue<1.1.0,>=0.0.7 (from spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/6c/f9/9a5658e2f56932e41eb264941f9a2cb7f3ce41a80cb36b2af6ab78e2f8af/catalogue-1.0.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.2)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.9.11)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /opt/anaconda3/lib/python3.7/site-packages (from thinc<7.4.0,>=7.3.0->spacy) (4.36.1)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /opt/anaconda3/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy) (0.23)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/anaconda3/lib/python3.7/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (0.6.0)\n",
      "Requirement already satisfied: more-itertools in /opt/anaconda3/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (7.2.0)\n",
      "Installing collected packages: murmurhash, cymem, preshed, srsly, plac, blis, wasabi, thinc, catalogue, spacy\n",
      "Successfully installed blis-0.4.1 catalogue-1.0.0 cymem-2.0.3 murmurhash-1.0.2 plac-1.1.3 preshed-3.0.2 spacy-2.2.3 srsly-1.0.1 thinc-7.3.1 wasabi-0.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12.0MB 2.3MB/s eta 0:00:01\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_lg\n",
    "nlp = en_core_web_lg.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the average color, we'll follow these steps:\n",
    "\n",
    "1. Parse the text into words\n",
    "2. Check every word to see if it names a color in our vector space. If it does, add it to a list of vectors.\n",
    "3. Find the average of that list of vectors.\n",
    "4. Find the color(s) closest to that average vector.\n",
    "\n",
    "The following cell performs steps 1-3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[147.44839067702551, 113.65371809100999, 100.13540510543841]\n"
     ]
    }
   ],
   "source": [
    "resp = requests.get('http://www.gutenberg.org/cache/epub/345/pg345.txt')\n",
    "dracula = nlp(resp.text)\n",
    "# use word.lower_ to normalize case\n",
    "drac_colors = [colors[word.lower_] for word in dracula if word.lower_ in colors]\n",
    "avg_color = meanv(drac_colors)\n",
    "print(avg_color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll pass the averaged color vector to the `closest()` function, yielding... well, it's just a brown mush, which is kinda what you'd expect from adding a bunch of colors together willy-nilly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['reddish grey',\n",
       " 'brownish grey',\n",
       " 'brownish',\n",
       " 'brown grey',\n",
       " 'mocha',\n",
       " 'grey brown',\n",
       " 'puce',\n",
       " 'dull brown',\n",
       " 'pinkish brown',\n",
       " 'dark taupe']"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closest(colors, avg_color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, here's what we get when we average the colors of Charlotte Perkins Gilman's classic *The Yellow Wallpaper*. The result definitely reflects the content of the story, so maybe we're on to something here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sickly yellow',\n",
       " 'piss yellow',\n",
       " 'puke yellow',\n",
       " 'vomit yellow',\n",
       " 'dirty yellow',\n",
       " 'mustard yellow',\n",
       " 'dark yellow',\n",
       " 'olive yellow',\n",
       " 'macaroni and cheese',\n",
       " 'pea']"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp = requests.get('http://www.gutenberg.org/cache/epub/1952/pg1952.txt')\n",
    "yellow = nlp(resp.text)\n",
    "wallpaper_colors = [colors[word.lower_] for word in yellow if word.lower_ in colors]\n",
    "avg_color = meanv(wallpaper_colors)\n",
    "closest(colors, avg_color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise for the reader: Use the vector arithmetic functions to rewrite a text, making it...\n",
    "\n",
    "* more blue (i.e., add `colors['blue']` to each occurrence of a color word); or\n",
    "* more light (i.e., add `colors['white']` to each occurrence of a color word); or\n",
    "* darker (i.e., attenuate each color. You might need to write a vector multiplication function to do this one right.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributional semantics\n",
    "\n",
    "In the previous section, the examples are interesting because of a simple fact: colors that we think of as similar are \"closer\" to each other in RGB vector space. In our color vector space, or in our animal cuteness/size space, you can think of the words identified by vectors close to each other as being *synonyms*, in a sense: they sort of \"mean\" the same thing. They're also, for many purposes, *functionally identical*. Think of this in terms of writing, say, a search engine. If someone searches for \"mauve trousers,\" then it's probably also okay to show them results for, say,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mauve trousers\n",
      "dusty rose trousers\n",
      "dusky rose trousers\n",
      "brownish pink trousers\n",
      "old pink trousers\n",
      "reddish grey trousers\n",
      "dirty pink trousers\n",
      "old rose trousers\n",
      "light plum trousers\n",
      "ugly pink trousers\n"
     ]
    }
   ],
   "source": [
    "for cname in closest(colors, colors['mauve']):\n",
    "    print(cname, \"trousers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all well and good for color words, which intuitively seem to exist in a multidimensional continuum of perception, and for our animal space, where we've written out the vectors ahead of time. But what about... arbitrary words? Is it possible to create a vector space for all English words that has this same \"closer in space is closer in meaning\" property?\n",
    "\n",
    "To answer that, we have to back up a bit and ask the question: what does *meaning* mean? No one really knows, but one theory popular among computational linguists, computer scientists and other people who make search engines is the [Distributional Hypothesis](https://en.wikipedia.org/wiki/Distributional_semantics), which states that:\n",
    "\n",
    "    Linguistic items with similar distributions have similar meanings.\n",
    "    \n",
    "What's meant by \"similar distributions\" is *similar contexts*. Take for example the following sentences:\n",
    "\n",
    "    It was really cold yesterday.\n",
    "    It will be really warm today, though.\n",
    "    It'll be really hot tomorrow!\n",
    "    Will it be really cool Tuesday?\n",
    "    \n",
    "According to the Distributional Hypothesis, the words `cold`, `warm`, `hot` and `cool` must be related in some way (i.e., be close in meaning) because they occur in a similar context, i.e., between the word \"really\" and a word indicating a particular day. (Likewise, the words `yesterday`, `today`, `tomorrow` and `Tuesday` must be related, since they occur in the context of a word indicating a temperature.)\n",
    "\n",
    "In other words, according to the Distributional Hypothesis, a word's meaning is just a big list of all the contexts it occurs in. Two words are closer in meaning if they share contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word vectors by counting contexts\n",
    "\n",
    "So how do we turn this insight from the Distributional Hypothesis into a system for creating general-purpose vectors that capture the meaning of words? Maybe you can see where I'm going with this. What if we made a *really big* spreadsheet that had one column for every context for every word in a given source text. Let's use a small source text to begin with, such as this excerpt from Dickens:\n",
    "\n",
    "    It was the best of times, it was the worst of times.\n",
    "\n",
    "Such a spreadsheet might look something like this:\n",
    "\n",
    "![dickens contexts](http://static.decontextualize.com/snaps/best-of-times.png)\n",
    "\n",
    "The spreadsheet has one column for every possible context, and one row for every word. The values in each cell correspond with how many times the word occurs in the given context. The numbers in the columns constitute that word's vector, i.e., the vector for the word `of` is\n",
    "\n",
    "    [0, 0, 0, 0, 1, 0, 0, 0, 1, 0]\n",
    "    \n",
    "Because there are ten possible contexts, this is a ten dimensional space! It might be strange to think of it, but you can do vector arithmetic on vectors with ten dimensions just as easily as you can on vectors with two or three dimensions, and you could use the same distance formula that we defined earlier to get useful information about which vectors in this space are similar to each other. In particular, the vectors for `best` and `worst` are actually the same (a distance of zero), since they occur only in the same context (`the ___ of`):\n",
    "\n",
    "    [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
    "    \n",
    "Of course, the conventional way of thinking about \"best\" and \"worst\" is that they're *antonyms*, not *synonyms*. But they're also clearly two words of the same kind, with related meanings (through opposition), a fact that is captured by this distributional model.\n",
    "\n",
    "### Contexts and dimensionality\n",
    "\n",
    "Of course, in a corpus of any reasonable size, there will be many thousands if not many millions of possible contexts. It's difficult enough working with a vector space of ten dimensions, let alone a vector space of a million dimensions! It turns out, though, that many of the dimensions end up being superfluous and can either be eliminated or combined with other dimensions without significantly affecting the predictive power of the resulting vectors. The process of getting rid of superfluous dimensions in a vector space is called [dimensionality reduction](https://en.wikipedia.org/wiki/Dimensionality_reduction), and most implementations of count-based word vectors make use of dimensionality reduction so that the resulting vector space has a reasonable number of dimensions (say, 100â€”300, depending on the corpus and application).\n",
    "\n",
    "The question of how to identify a \"context\" is itself very difficult to answer. In the toy example above, we've said that a \"context\" is just the word that precedes and the word that follows. Depending on your implementation of this procedure, though, you might want a context with a bigger \"window\" (e.g., two words before and after), or a non-contiguous window (skip a word before and after the given word). You might exclude certain \"function\" words like \"the\" and \"of\" when determining a word's context, or you might [lemmatize](https://en.wikipedia.org/wiki/Lemmatisation) the words before you begin your analysis, so two occurrences with different \"forms\" of the same word count as the same context. These are all questions open to research and debate, and different implementations of procedures for creating count-based word vectors make different decisions on this issue.\n",
    "\n",
    "### GloVe vectors\n",
    "\n",
    "But you don't have to create your own word vectors from scratch! Many researchers have made downloadable databases of pre-trained vectors. One such project is Stanford's [Global Vectors for Word Representation (GloVe)](https://nlp.stanford.edu/projects/glove/). These 300-dimensional vectors are included with spaCy, and they're the vectors we'll be using for the rest of this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word vectors in spaCy\n",
    "\n",
    "Okay, let's have some fun with real word vectors. We're going to use the GloVe vectors that come with spaCy to creatively analyze and manipulate the text of Bram Stoker's *Dracula*. First, make sure you've got `spacy` imported:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hour"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dracula[560]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.0349e-02,  1.9876e-01,  4.6758e-01,  1.1855e-01,  4.9996e-01,\n",
       "       -3.1519e-01, -3.2802e-01, -1.7293e-01,  2.5819e-01,  2.2362e+00,\n",
       "        6.9453e-02,  3.0208e-01, -2.3887e-01, -1.2615e-01, -1.1964e-01,\n",
       "        2.7423e-01, -4.4399e-01,  1.4778e+00, -2.0461e-01, -6.0527e-01,\n",
       "       -3.9626e-01,  1.8723e-01, -3.6010e-01, -3.5477e-01,  1.2472e-02,\n",
       "        1.5440e-02,  1.7229e-02,  1.3381e-01,  5.4751e-01, -1.5930e-01,\n",
       "        2.5280e-01,  1.4952e-01, -4.2010e-01, -4.9675e-01,  1.2056e-02,\n",
       "        2.2982e-01, -1.4856e-01,  4.0397e-01, -9.7048e-03,  6.8103e-01,\n",
       "        8.9441e-02,  1.4817e-01,  3.6700e-02,  1.3173e-01, -2.8417e-01,\n",
       "        1.9149e-01, -3.7163e-01,  2.3378e-01,  2.1285e-01, -6.2602e-01,\n",
       "        2.6401e-01, -3.7330e-01,  5.0246e-01,  2.4712e-01, -6.9188e-01,\n",
       "        9.7506e-02,  2.1911e-01,  5.0629e-02,  5.0400e-01, -9.6926e-03,\n",
       "       -6.5704e-02, -7.3404e-02, -1.7547e-02, -9.3765e-02,  1.7430e-01,\n",
       "       -7.8283e-02,  4.0286e-01,  2.5159e-01, -3.3984e-01,  2.5716e-01,\n",
       "        9.1046e-02, -2.1603e-01,  4.8311e-01, -2.2661e-01,  1.7119e-01,\n",
       "       -6.5351e-03,  5.6783e-01,  4.0862e-01, -2.7525e-01,  5.7352e-01,\n",
       "        4.6801e-01,  1.2614e-01, -5.3857e-02, -4.2848e-02,  1.8734e-03,\n",
       "        1.0420e-01,  1.3474e-01, -8.3823e-03,  4.9371e-01, -4.5639e-02,\n",
       "        1.0354e-01, -1.9002e-01, -6.8882e-02, -1.6116e-01, -1.9521e-02,\n",
       "       -1.1372e-01, -2.9018e-01,  2.3664e-01,  1.0448e-01, -1.7466e-01,\n",
       "        1.4709e-01, -8.8437e-02, -2.7644e-02, -9.3012e-02, -2.6267e-01,\n",
       "       -3.6947e-01,  2.1025e-01, -3.5502e-01,  1.4227e-01, -3.2055e-01,\n",
       "        4.2555e-01, -4.3871e-01,  6.6931e-01,  6.2431e-01,  6.5596e-01,\n",
       "       -3.8732e-01,  2.0560e-01, -4.3082e-03,  1.9976e-01,  6.7030e-01,\n",
       "        6.3020e-02, -7.5632e-01, -5.2256e-02, -5.2427e-01, -4.1666e-01,\n",
       "       -1.3406e-01, -5.0339e-01,  1.2555e-01, -1.3530e-01, -4.3696e-01,\n",
       "        6.1185e-01, -1.3352e-01,  5.6203e-01,  3.7474e-01, -1.0342e-02,\n",
       "       -2.7341e-01,  2.5612e-01,  2.2055e-01, -7.4698e-02, -3.7033e-02,\n",
       "       -1.1634e+00, -6.3841e-02,  2.1730e-01, -2.1691e-01,  4.6448e-01,\n",
       "        2.8872e-01, -7.2510e-01,  2.1890e-01,  1.4639e-01, -1.2146e-01,\n",
       "        2.4751e-01, -1.5816e-02,  1.1244e-01, -2.3691e-01, -9.6946e-02,\n",
       "       -3.6056e-01,  2.7389e-01,  4.2751e-03, -6.2020e-02,  6.1985e-02,\n",
       "       -4.7747e-01, -2.2261e-01, -6.0253e-02,  4.3381e-02,  5.9486e-01,\n",
       "        1.4892e-01,  1.0725e-01,  6.7153e-03, -1.6186e-01,  1.1876e-01,\n",
       "        2.1581e-01, -1.6140e-01, -1.0863e-01, -4.5024e-01, -6.3666e-01,\n",
       "        1.1189e-01,  1.1810e-01, -7.5732e-01, -9.5770e-02, -6.6534e-01,\n",
       "       -7.5426e-02, -5.1346e-01, -8.6555e-02,  2.8470e-01,  8.5798e-02,\n",
       "        1.2448e-01, -2.5061e-01,  1.1711e-01,  4.6204e-01,  2.2134e-01,\n",
       "       -2.6761e-01,  5.3233e-01,  5.5413e-02,  1.5100e-01,  3.2516e-01,\n",
       "        2.4115e-01, -9.8404e-04, -2.2876e-01,  8.4751e-02, -5.8545e-01,\n",
       "        1.8630e-01,  1.7896e-01,  2.7423e-01,  6.4533e-02, -8.0051e-02,\n",
       "        3.2651e-01,  3.2636e-01, -2.6082e-01,  9.1919e-01,  2.5264e-01,\n",
       "        2.8629e-01,  1.8428e-02,  3.3729e-01, -1.2899e-01, -1.6540e-02,\n",
       "        7.9580e-02, -1.4911e-01,  1.2522e-02,  2.7087e-01,  6.0658e-01,\n",
       "        2.0410e-01, -5.6810e-01, -2.8661e-01,  1.3880e-01, -2.2006e-01,\n",
       "       -1.7054e-04,  9.3404e-02,  2.9099e-01, -5.7977e-01,  3.1251e-02,\n",
       "        7.6602e-01,  7.5468e-02,  2.0025e-01,  7.2768e-01,  4.8083e-02,\n",
       "       -2.4703e-01, -1.3836e-01, -1.3635e-01, -6.1955e-01,  4.1529e-01,\n",
       "        1.6915e-01, -1.4837e-01, -2.9181e-01, -8.8142e-02, -3.4713e-01,\n",
       "       -2.6418e-01, -5.1040e-01,  4.3677e-01, -4.7698e-01, -3.6635e-01,\n",
       "       -5.3522e-01, -3.0290e-01,  3.5682e-01,  4.8952e-02,  1.2413e-01,\n",
       "        9.6399e-01, -4.3357e-02, -6.6789e-01, -3.3742e-02,  9.8217e-02,\n",
       "        6.1047e-01,  3.4802e-01, -2.6448e-02, -5.0817e-01, -2.5553e-01,\n",
       "       -1.5906e-01, -1.3748e-01,  4.5577e-01,  1.7709e-02, -2.2049e-01,\n",
       "        8.0129e-02, -3.7275e-01, -4.6633e-03, -2.1606e-01, -3.7695e-01,\n",
       "        6.4245e-01, -2.3999e-01,  8.9938e-02,  3.9249e-01,  6.8738e-01,\n",
       "        3.4545e-01, -3.8479e-01,  3.0420e-01,  3.6635e-01, -2.7650e-01,\n",
       "        4.9682e-02, -5.9223e-02, -3.8312e-01,  2.2649e-01,  8.6374e-01,\n",
       "        5.3798e-01,  2.4134e-02,  2.3418e-01, -7.8956e-02,  4.5423e-02,\n",
       "        4.5282e-03,  4.2914e-02,  2.1358e-02, -5.9107e-01,  1.7641e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_.vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# previously we've used the _sm model, which doesn't include all vectors.\n",
    "# !pip3 install https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.0/en_core_web_lg-2.2.0.tar.gz\n",
    "import en_core_web_lg\n",
    "nlp = en_core_web_lg.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = requests.get('http://www.gutenberg.org/cache/epub/345/pg345.txt')\n",
    "dracula = nlp(resp.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'Barbie loves deoxyribonucleic asdfasdfasdfasdf is really spunky'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = nlp(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(words)\n",
    "dna = words[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ADV'"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dna.pos_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the cell below creates a list of unique words (or tokens) in the text, as a list of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all of the words in the text file\n",
    "tokens = list(set([w.text for w in dracula if w.is_alpha]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the vector of any word in spaCy's vocabulary using the `vocab` attribute, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.9304e-01,  1.2459e-01,  2.6142e-01,  3.8393e-02,  1.4018e-01,\n",
       "        5.7285e-01, -5.0449e-01,  1.5094e-01,  7.3356e-02,  2.3308e-01,\n",
       "       -1.6048e-01, -5.1184e-01, -1.4028e-01, -3.0110e-01, -4.3446e-01,\n",
       "       -2.8257e-02,  1.9100e-03,  6.5157e-01,  1.4855e-01, -3.7255e-01,\n",
       "       -4.6619e-01, -1.7223e-01, -5.1794e-01, -2.5453e-01, -2.6785e-01,\n",
       "       -6.7776e-02, -3.0085e-01, -3.3212e-01, -1.5862e-01, -1.2336e-01,\n",
       "       -3.8935e-01, -3.5551e-01, -3.6182e-01,  2.3197e-02, -1.7486e-01,\n",
       "        2.2345e-01,  6.7557e-01,  9.6939e-03, -9.5640e-02,  1.2073e-01,\n",
       "       -1.2016e-01, -1.6161e-01,  1.0661e-01, -4.5233e-01,  4.5991e-01,\n",
       "        1.0367e-02, -1.0842e-01, -3.6163e-01,  2.5105e-01,  9.5780e-03,\n",
       "       -6.1363e-01,  2.7494e-01,  2.1783e-01, -1.0663e-01, -9.5146e-03,\n",
       "        2.7570e-01,  2.5888e-02, -5.6749e-02,  6.7667e-03,  5.3735e-01,\n",
       "       -1.2224e-01,  8.8448e-02,  4.6094e-01,  4.6881e-02, -6.6557e-01,\n",
       "       -3.9493e-01,  2.7362e-01,  1.2302e-01,  2.8594e-01, -2.5274e-02,\n",
       "       -8.8458e-02, -6.5280e-01, -1.9723e-01,  2.4883e-01, -7.0509e-01,\n",
       "       -3.9357e-01,  2.1244e-01,  2.8522e-01, -1.3837e-01, -8.0399e-01,\n",
       "        4.0477e-01,  5.5522e-01,  1.3422e-01,  1.7541e-01, -2.4896e-01,\n",
       "       -3.7945e-02,  9.0174e-01,  5.7262e-01,  5.8552e-01, -2.7852e-02,\n",
       "       -3.7543e-01, -2.2329e-01, -1.7649e-01, -2.7149e-01, -1.4442e-01,\n",
       "        1.1173e-01, -7.5823e-01, -3.2636e-02,  3.8259e-01, -4.7711e-01,\n",
       "       -6.4061e-02,  7.7816e-01,  5.2918e-01, -8.8252e-02, -4.8766e-01,\n",
       "       -1.0542e+00, -1.7567e-01,  5.0561e-01, -3.2961e-01,  6.4652e-02,\n",
       "        1.6991e-01,  6.4564e-01, -1.1113e-01, -2.7329e-01,  4.5151e-01,\n",
       "        2.0905e-01,  5.5951e-01, -5.6235e-01,  6.4065e-01,  2.6138e-01,\n",
       "       -7.6238e-02, -5.7198e-01,  5.7286e-01, -6.4884e-01, -3.5580e-02,\n",
       "       -2.9065e-01, -2.3659e-01, -1.5837e-01, -3.8830e-01,  3.5099e-01,\n",
       "        1.2338e-01, -7.7062e-01, -3.2580e-01,  5.4685e-01,  9.9670e-02,\n",
       "       -1.7511e-01,  5.9578e-01,  5.7704e-02,  3.6616e-01,  7.9128e-02,\n",
       "       -2.1015e+00,  2.9241e-01,  1.6590e-01, -1.1812e-01,  3.6034e-01,\n",
       "        2.1378e-01, -3.8785e-02, -1.6689e-01,  2.5772e-01,  1.4423e-01,\n",
       "       -3.2560e-01, -4.9781e-01, -2.4736e-01, -4.5972e-01,  5.3241e-03,\n",
       "       -3.8934e-01, -5.6316e-01, -2.1875e-02,  4.6312e-01,  2.7124e-01,\n",
       "        8.2626e-02,  1.2798e-01, -1.7966e-01, -4.0142e-01,  3.9830e-01,\n",
       "        4.7246e-01, -1.7184e-02,  3.0340e-01,  7.2356e-01,  6.2398e-03,\n",
       "        1.1034e-01,  4.3798e-01, -1.8209e-01,  7.9000e-01, -8.2937e-01,\n",
       "       -2.5152e-01, -3.5818e-01,  1.9120e-01,  2.6807e-01, -4.8448e-02,\n",
       "        4.4986e-01, -1.9426e-01, -4.1831e-01, -4.7271e-01,  1.8365e-02,\n",
       "       -5.3575e-02,  2.0164e-01,  1.4613e-01, -5.2150e-01, -3.5136e-01,\n",
       "       -5.6628e-01,  5.6513e-01,  1.5970e-01,  4.9475e-01,  7.8591e-02,\n",
       "        5.8238e-01,  5.5110e-01,  4.7882e-01,  5.1979e-02, -5.8825e-01,\n",
       "       -3.1104e-01,  1.9566e-01, -3.4389e-01,  5.2565e-01,  2.7204e-01,\n",
       "       -1.9269e-01,  4.0240e-01,  3.5336e-01,  1.1571e-01, -1.7639e-01,\n",
       "        3.6528e-01,  4.7704e-01, -7.1949e-02, -2.3349e-01, -3.6303e-01,\n",
       "        1.8091e-01, -6.1703e-01,  1.0734e-01,  2.0010e-01,  4.5323e-02,\n",
       "       -1.4448e-01,  2.3747e-01, -1.0277e-01, -5.3455e-01,  1.3774e-01,\n",
       "       -2.1872e-01,  2.9646e-01,  8.5221e-01,  1.0667e-01,  8.3190e-02,\n",
       "        2.1841e-01, -2.0192e-01, -9.4416e-02,  1.7515e-01,  4.4191e-01,\n",
       "        5.8381e-01, -3.2412e-01, -2.3751e-01,  6.5186e-01,  3.9159e-01,\n",
       "        2.4443e-01, -6.5128e-01, -3.2916e-01, -5.9612e-01, -2.8962e-01,\n",
       "        1.2222e-01, -2.6307e-02,  1.1961e-01, -2.7490e-01,  2.4727e-01,\n",
       "       -1.0389e-01,  3.8307e-01, -5.0076e-01,  4.6286e-01,  4.6927e-01,\n",
       "       -3.5431e-01,  5.9146e-01,  2.0201e-01,  8.0544e-01,  2.7008e-01,\n",
       "       -5.9786e-02, -2.7738e-01, -2.3822e-01, -5.1769e-02,  2.1425e-01,\n",
       "        2.5170e-01,  1.8265e-01, -2.4323e-01, -3.2770e-01,  1.5040e-02,\n",
       "       -3.8119e-01, -5.2011e-02,  4.2345e-01,  6.3603e-01,  5.0159e-01,\n",
       "       -1.6942e-01, -2.3502e-01, -2.0667e-01, -9.0480e-02, -9.7789e-01,\n",
       "       -2.8939e-01,  2.2706e-01, -1.2613e-01,  4.4867e-02,  3.8184e-01,\n",
       "       -3.1982e-01, -1.4018e-01, -1.3953e-02, -4.4110e-01, -4.3323e-01,\n",
       "        4.6663e-01,  2.8850e-01, -8.1727e-01, -4.4984e-01,  8.9386e-02,\n",
       "        9.0084e-01, -5.8482e-02,  5.1920e-02,  1.5626e-02, -7.1096e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab['alligator'].vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of convenience, the following function gets the vector of a given string from spaCy's vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec(s):\n",
    "    return nlp.vocab[s].vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine similarity and finding closest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell shows that the cosine similarity between `dog` and `puppy` is larger than the similarity between `trousers` and `octopus`, thereby demonstrating that the vectors are working how we expect them to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(cosine_similarity(vec('dog').reshape(1, -1), vec('puppy').reshape(1, -1))[0][0] > \n",
    "    cosine_similarity(vec('trousers').reshape(1, -1), vec('octopus').reshape(1, -1))[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell defines a function that iterates through a list of tokens and returns the token whose vector is most similar to a given vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_closest(token_list, vec_to_check, n=10):\n",
    "    return sorted(token_list,\n",
    "                  key=lambda x: cosine_similarity(vec_to_check.reshape(1, -1), \n",
    "                                                  vec(x).reshape(1, -1))[0][0],\n",
    "                  reverse=True)[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this function, we can get a list of synonyms, or words closest in meaning (or distribution, depending on how you look at it), to any arbitrary word in spaCy's vocabulary. In the following example, we're finding the words in *Dracula* closest to \"basketball\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tennis',\n",
       " 'coach',\n",
       " 'game',\n",
       " 'teams',\n",
       " 'Junior',\n",
       " 'junior',\n",
       " 'Team',\n",
       " 'school',\n",
       " 'boys',\n",
       " 'leagues']"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what's the closest equivalent of basketball?\n",
    "spacy_closest(tokens, vec(\"basketball\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fun with spaCy, Dracula, and vector arithmetic\n",
    "\n",
    "Now we can start doing vector arithmetic and finding the closest words to the resulting vectors. For example, what word is closest to the halfway point between day and night?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['night',\n",
       " 'Day',\n",
       " 'day',\n",
       " 'evening',\n",
       " 'Evening',\n",
       " 'morning',\n",
       " 'Morning',\n",
       " 'afternoon',\n",
       " 'nights',\n",
       " 'Nights']"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# halfway between day and night\n",
    "spacy_closest(tokens, np.array(meanv([vec(\"day\"), vec(\"night\")])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variations of `night` and `day` are still closest, but after that we get words like `evening` and `morning`, which are indeed halfway between day and night!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the closest words in _Dracula_ to \"wine\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wine',\n",
       " 'beer',\n",
       " 'bottle',\n",
       " 'Drink',\n",
       " 'drink',\n",
       " 'cellar',\n",
       " 'fruit',\n",
       " 'bottles',\n",
       " 'brandy',\n",
       " 'taste']"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_closest(tokens, vec(\"wine\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you subtract \"alcohol\" from \"wine\" and find the closest words to the resulting vector, you're left with simply a lovely dinner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wine',\n",
       " 'cellar',\n",
       " 'exquisite',\n",
       " 'fabulous',\n",
       " 'splendid',\n",
       " 'magnificent',\n",
       " 'delightful',\n",
       " 'Dinner',\n",
       " 'dinner',\n",
       " 'sparkling']"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_closest(tokens, np.array(subtractv(vec(\"wine\"), vec(\"alcohol\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The closest words to \"water\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['water',\n",
       " 'waters',\n",
       " 'salt',\n",
       " 'Salt',\n",
       " 'pond',\n",
       " 'dry',\n",
       " 'liquid',\n",
       " 'ocean',\n",
       " 'boiling',\n",
       " 'heat']"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_closest(tokens, vec(\"water\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if you add \"frozen\" to \"water,\" you get \"ice\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['water',\n",
       " 'cold',\n",
       " 'ice',\n",
       " 'salt',\n",
       " 'Salt',\n",
       " 'dry',\n",
       " 'fresh',\n",
       " 'liquid',\n",
       " 'boiling',\n",
       " 'milk']"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_closest(tokens, np.array(addv(vec(\"water\"), vec(\"frozen\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can even do analogies! For example, the words most similar to \"grass\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['grass',\n",
       " 'lawn',\n",
       " 'trees',\n",
       " 'greens',\n",
       " 'grassy',\n",
       " 'GARDEN',\n",
       " 'garden',\n",
       " 'sand',\n",
       " 'foliage',\n",
       " 'tree']"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_closest(tokens, vec(\"grass\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you take the difference of \"blue\" and \"sky\" and add it to grass, you get the analogous word (\"green\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['grass',\n",
       " 'GREEN',\n",
       " 'Green',\n",
       " 'green',\n",
       " 'yellow',\n",
       " 'Red',\n",
       " 'red',\n",
       " 'purple',\n",
       " 'lawn',\n",
       " 'pink']"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# analogy: blue is to sky as X is to grass\n",
    "blue_to_sky = subtractv(vec(\"blue\"), vec(\"sky\"))\n",
    "spacy_closest(tokens, np.array(addv(blue_to_sky, vec(\"grass\"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further resources\n",
    "\n",
    "* [Word2vec](https://en.wikipedia.org/wiki/Word2vec) is another procedure for producing word vectors which uses a predictive approach rather than a context-counting approach. [This paper](http://clic.cimec.unitn.it/marco/publications/acl2014/baroni-etal-countpredict-acl2014.pdf) compares and contrasts the two approaches. (Spoiler: it's kind of a wash.)\n",
    "* If you want to train your own word vectors on a particular corpus, the popular Python library [gensim](https://radimrehurek.com/gensim/) has an implementation of Word2Vec that is relatively easy to use. [There's a good tutorial here.](https://rare-technologies.com/word2vec-tutorial/)\n",
    "* When you're working with vector spaces with high dimensionality and millions of vectors, iterating through your entire space calculating cosine similarities can be a drag. I use [Annoy](https://pypi.python.org/pypi/annoy) to make these calculations faster, and you should consider using it too."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
